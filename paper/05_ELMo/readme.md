## ğŸ¤— Result

### environment
```
mlflow == 1.15.0
torch == 1.7.1
torchtext == 0.8.1
pytorch-lightning == 1.2.8
```


### result
#### pretraining
- data : [petition data](https://github.com/lovit/petitions_archive)
- task : Language modeling

```
python run_main.py
```
![image](https://user-images.githubusercontent.com/46675408/120097868-c5db1f00-c16d-11eb-91fa-41763c01a640.png)

|train loss|train accuracy|
|---|---|
|0.343|0.55|


#### finetuning
- data : [KLUE](https://klue-benchmark.com/tasks/66/leaderboard/task)
- task : topic classification

```
python run.py
```

##### accuracy
![image](https://user-images.githubusercontent.com/46675408/120183336-82a2ae00-c24a-11eb-8937-3ce061567e93.png)
|train accuracy|valid accuracy|
|---|---|
|0.95|0.79|

##### fscore
![image](https://user-images.githubusercontent.com/46675408/120183572-d614fc00-c24a-11eb-9aa5-5a5069c7bf29.png)
|train f-score|valid f-score|
|---|---|
|0.9319|0.7639|

#### ablation study
. gamma vector added :

![image](https://user-images.githubusercontent.com/46675408/120253013-35155800-c2c1-11eb-943f-23711215fa93.png)
|gamma vector x valid accuracy|gamma vector o valid accuracy|
|---|---|
|0.74|0.79|


## ğŸ¤” Paper review
![image](https://user-images.githubusercontent.com/46675408/121776935-87f8e480-cbca-11eb-9a25-04e002dc721b.png)

**2) (ìŠ¬ë™ìœ¼ë¡œ ì´ë¯¸ í† ë¡ ì„ í–ˆì§€ë§Œ ê·¸ë˜ë„) ì´í•´ê°€ ì•ˆ ê°€ëŠ” ë¶€ë¶„, ì´í•´ê°€ ì•ˆ ê°€ëŠ” ì´ìœ (ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**

CNNì„ ì–´ë–»ê²Œ í–ˆë‹¤ëŠ” ê±´ì§€? ì € Srivastava ë…¼ë¬¸ì´ë‘ ë˜‘ê°™ì´ í•˜ë©´ ë˜ëŠ”ê±´ê°€? 2048 character n-gramì´ë¼ëŠ”ê²Œ ë¬´ìŠ¨ ëœ»ì¸ì§€
-> cnn filterê°€ 2048ê°œë‹¤ 

CNN-BIG-LSTM in Jozefowicz et al. Â´(2016). The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers (Srivastava et al., 2015) and a linear projection down to a 512 representation.

= ì•„ë˜ ë¬¸ì¥ì´ forward, backwardì˜ í† í° í‘œí˜„ê³¼ softmax layerë¥¼ ê³µìœ í•œë‹¤ê³  í•˜ëŠ” ê²ƒ ê°™ì€ë°, softmax layerë¥¼ ê³µìœ í•œë‹¤ëŠ”ê±°ëŠ” ë§ˆì§€ë§‰ FCNê¹Œì§€ë¥¼ ì˜ë¯¸í•œë‹¤ëŠ”ê±´ê°€? <BR>We tie the parameters for both the token representation (Î˜x) and Softmax layer (Î˜s) in the forward and backward direction while maintaining separate parameters for the LSTMs in each direction. <BR>
-> forward, backward concatí•´ì„œ FCN + softmax ë ˆì´ì–´ë¡œ ì˜ˆì¸¡.

= ELMo fine tuningì‹œ ì–´ë””ê¹Œì§€ ? LMì„ í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ freezeí•˜ê³  ELMotaskì™€ xkì— ëŒ€í•œ ì„ë² ë”©ì„ concatí•œë‹¤ìŒì— RNNë§Œ í•™ìŠµë˜ëŠ” ê±´ê°€? 

To add ELMo to the supervised model, we first freeze the weights of the biLM and then concatenate the ELMo vector ELMotask k with xk and pass the ELMo enhanced representation [xk; ELMotask k] into the task RNN

= 3.3, 5.2ì—ì„œ ELMoë¥¼ inputë¿ ì•„ë‹ˆë¼ outputì— ë„£ìœ¼ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§„ë‹¤ëŠ” ê²Œ ë¬´ìŠ¨ ë§ì¼ê¹Œ? 

â†’ rnnì˜ outputìœ¼ë¡œ ë‚˜ê°ˆë•Œ hiddenì— elmo vectorë¥¼ concatí•˜ê² ë‹¤

including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer, but for SRL (and coreference resolution, not shown) performance is highest when it is included at just the input layer

**3) ì¬ë°Œì—ˆë˜ ë¶€ë¶„**

. í‘œ 4ì—ì„œ gloveëŠ” í† í° í•˜ë‚˜ë‹¹ í‘œí˜„ í•˜ë‚˜ì—¬ì„œ ì—¬ëŸ¬ í’ˆì‚¬ì˜ í† í°ì„ ë‹´ì§€ë§Œ, ELMoì˜ í‘œí˜„ì€ ê·¸ëŸ° í’ˆì‚¬ ë“±ì˜ ì •ë³´ê¹Œì§€ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê²ƒ
. ì—¬ëŸ¬ ê°€ì§€ ë‹¹ì‹œ ë…¼ë¬¸ë“¤ì„ ì§¬ë½•í•œ ì 
. ë ˆì´ì–´ë§ˆë‹¤ íˆë“  ë²¡í„°ë¥¼ ì‚¬ìš©í•´ì„œ ë‹¨ì–´í‘œí˜„ì„ í•œ ì . ì œëª©ì´ ì²˜ìŒì—” ìŒ©ëš±ë§ì•˜ëŠ”ë° ì½ë‹¤ë³´ë‹ˆ ê·¸ë ‡êµ¬ë‚˜..ì‹¶ìŒ 

**4) ë…¼ë¬¸ êµ¬í˜„ ì‹œ ì£¼ì˜í•´ì•¼í•  ê²ƒ ê°™ì€ ë¶€ë¶„(ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**

. residual connection - LSTM<BR>
. L2 regularization<BR>
. CNNëª¨ë¸ì—ì„œ highway <BR>
. ELMo task vector ë§Œë“œëŠ” ë¶€ë¶„<BR>
. fine-tuning taskêµ¬í•˜ê¸°, ë°ì´í„°ì…‹ êµ¬í•˜ê¸°<BR>

## ğŸ¤« ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê²Œ êµ¬í˜„í•œ ë¶€ë¶„
. dataset : [petition data](https://github.com/lovit/petitions_archive)<BR>
. LSTM residual connection ë¶€ë¶„ ìƒëµí•¨<BR>
. L2 norm ë“± ... 
  
## ğŸ¤­ ë…¼ë¬¸ êµ¬í˜„í•˜ë©´ì„œ ë°°ìš´ ì  / ëŠë‚€ ì 
- ë…¼ë¬¸ ì²˜ìŒë¶€í„° êµ¬í˜„í•œ ê²½í—˜!
- í•œêµ­ì–´ ë°ì´í„°ë¡œ ë‚˜ë¦„(!) ëŒ€í˜• ëª¨ë¸ì„ í•™ìŠµí•œ ê²½í—˜
- tokenìœ¼ë¡œ í•œë²ˆ, charë¡œ í•œë²ˆ ì˜ë¼ì•¼í•˜ëŠ” CNN inputì„ ë§Œë“¤ê¸° ìœ„í•œ ê³ êµ°ë¶„íˆ¬(torchtextì˜ nestedField ì“°ë©´ ëœë‹¤ê³  í•˜ë”ë¼)
- ì‚¬ìš©ì torchtextì¸ `torch8text`ì—ì„œ Field ê°œì„ , labelField ë§Œë“¦
- `mlflow`ì™€ `pytorch-lightning`ì‚¬ìš©ì„ ë§ë³´ê³  ì‹ ì„¸ê³„ë¥¼ ê²½í—˜í•¨
- `nn.LSTM`ì˜ `bidirectional=True` argumentì— ëŒ€í•œ ê³ ì°°
- pretraining - finetuningìœ¼ë¡œ ë„˜ê¸¸ë•Œ vocab, í† í¬ë‚˜ì´ì € ì €ì¥ ë“±ì´ ì™œ í•„ìš”í•œì§€ ì•Œê²Œ ë¨
- í•™ìŠµì´ ë„ˆë¬´ ì•ˆë˜ë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í• ê²Œ ì•„ë‹ˆë¼ ëª¨ë¸ ì•„í‚¤í…ì³ë‚˜ ë°ì´í„° ì¸í’‹ì„ í™•ì¸í•˜ì
- ê·¸ëƒ¥ ì „ì²´ í•©ì³ì„œ ë””ë²„ê¹…í•˜ì§€ë§ê³  dataset, model, trainer í•˜ë‚˜í•˜ë‚˜ì”© ë””ë²„ê¹…í•˜ì
- `.pt`íŒŒì¼ë¡œ ì €ì¥í•˜ë©´ ëª¨ë¸ êµ¬ì¡°ë¥¼ ì €ì¥í•  í•„ìš” ì—†ìŒ(torchê¶Œì¥ì‚¬í•­ì€ ì•„ë‹˜)
- í•™ìŠµ ì´ˆê¸°ì—ëŠ” 0 0 0 0 0 í•˜ë‚˜ë¡œ ì˜ˆì¸¡í•˜ëŠ” ê²½í–¥ì„±ì´ ìˆìŒ. ëª¨ë¸ì˜ ì˜ëª»ì´ ì•„ë‹˜
- í•œêµ­ì–´ ë°ì´í„°ì— chrë¡œ paddingê¹Œì§€ í•˜ë‹ˆ CNNì´ ê·¸ë ‡ê²Œ íš¨ê³¼ê°€ ìˆì§€ëŠ” ì•Šì•˜ìŒ
