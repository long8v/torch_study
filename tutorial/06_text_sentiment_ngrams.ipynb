{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchText로 텍스트 분류하기\n",
    "==================================\n",
    "**번역**: `김강민 <https://github.com/gangsss>` , `김진현 <https://github.com/lewhe0>`\n",
    "\n",
    "이 튜토리얼에서는 ``torchtext`` 에 포함되어 있는 텍스트 분류\n",
    "데이터셋의 사용 방법을 살펴 봅니다. 데이터셋은 다음을 포함합니다.\n",
    "\n",
    "   - AG_NEWS,\n",
    "   - SogouNews,\n",
    "   - DBpedia,\n",
    "   - YelpReviewPolarity,\n",
    "   - YelpReviewFull,\n",
    "   - YahooAnswers,\n",
    "   - AmazonReviewPolarity,\n",
    "   - AmazonReviewFull\n",
    "\n",
    "이 예제에서는 ``TextClassification`` 의 데이터셋들 중 하나를 이용해 분류를 위한\n",
    " 지도 학습 알고리즘을 훈련하는 방법을 보여줍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngrams를 이용하여 데이터 불러오기\n",
    "-----------------------------------\n",
    "\n",
    "Bag of ngrams 피쳐는 지역(local) 단어 순서에 대한 부분적인 정보를 포착하기 위해 적용합니다.\n",
    "실제 상황에서는 bi-gram이나 tri-gram은 단 하나의 단어를 이용하는 것보다 더 많은 이익을 주기 때문에 적용됩니다.\n",
    "예를 들면 다음과 같습니다.\n",
    "\n",
    "   \"load data with ngrams\"\n",
    "   Bi-grams 결과: \"load data\", \"data with\", \"with ngrams\"\n",
    "   Tri-grams 결과: \"load data with\", \"data with ngrams\"\n",
    "\n",
    "``TextClassification`` 데이터셋은 ngrams method을 지원합니다. ngrams을 2로 설정하면,\n",
    "데이터셋 안의 예제 텍스트는 각각의(single) 단어들에 bi-grams 문자열이 더해진 리스트가 될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:05, 22818.34lines/s]\n",
      "120000lines [00:11, 10624.65lines/s]\n",
      "7600lines [00:00, 10698.79lines/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir('./.data'):\n",
    "    os.mkdir('./.data')\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, tensor([    572,     564,       2,    2326,   49106,     150,      88,       3,\n",
      "           1143,      14,      32,      15,      32,      16,  443749,       4,\n",
      "            572,     499,      17,      10,  741769,       7,  468770,       4,\n",
      "             52,    7019,    1050,     442,       2,   14341,     673,  141447,\n",
      "         326092,   55044,    7887,     411,    9870,  628642,      43,      44,\n",
      "            144,     145,  299709,  443750,   51274,     703,   14312,      23,\n",
      "        1111134,  741770,  411508,  468771,    3779,   86384,  135944,  371666,\n",
      "           4052]))\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 정의하기\n",
    "-------------\n",
    "\n",
    "우리의 모델은\n",
    "`EmbeddingBag`레이어와 선형 레이어로 구성됩니다 (아래 그림 참고).\n",
    "``nn.EmbeddingBag``는 임베딩들로 구성된 '가방'의 평균을 계산합니다.\n",
    "이때 텍스트(text)의 각 원소는 그 길이가 다를 수 있습니다. 텍스트의\n",
    "길이는 오프셋(offset)에 저장되어 있으므로 여기서 ``nn.EmbeddingBag``\n",
    "에 패딩을 사용할 필요는 없습니다.\n",
    "\n",
    "덧붙여서, ``nn.EmbeddingBag`` 은 임베딩의 평균을 즉시 계산하기 때문에,\n",
    "텐서들의 시퀀스를 처리할 때 성능 및 메모리 효율성 측면에서의 장점도\n",
    "갖고 있습니다.\n",
    "\n",
    "![](../_static/img/text_sentiment_ngrams_model.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>torch.nn.EmbeddingBag</b>(num_embeddings: int, embedding_dim: int, max_norm: Optional\\[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, _weight: Optional\\[torch.Tensor] = None, include_last_offset: bool = False)<br>\n",
    "Computes sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_embeddings (int) – size of the dictionary of embeddings\n",
    "- embedding_dim (int) – the size of each embedding vector\n",
    "- max_norm (float, optional) – If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm.\n",
    "- norm_type (float, optional) – The p of the p-norm to compute for the max_norm option. Default 2.\n",
    "- scale_grad_by_freq (boolean, optional) – if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\".\n",
    "- mode (string, optional) – \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. \"sum\" computes the weighted sum, taking per_sample_weights into consideration. \"mean\" computes the average of the values in the bag, \"max\" computes the max value over each bag. Default: \"mean\"\n",
    "- sparse (bool, optional) – if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\".\n",
    "- include_last_offset (bool, optional) – if True, offsets has one additional element, where the last element is equivalent to the size of indices. This matches the CSR format. Note: this option is currently only supported when mode=\"sum\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Inputs</b>: input (LongTensor), offsets (LongTensor, optional), and\n",
    "per_index_weights (Tensor, optional)\n",
    "- If input is 2D of shape (B, N),<br>\n",
    "it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.\n",
    "- If input is 1D of shape (N),<br>\n",
    "it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3658,  0.9116, -4.7369],\n",
       "        [-0.2858, -0.6938, -0.1603]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') \n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([0,1,2,4,5,4,3,2,9])\n",
    "offsets = torch.LongTensor([0,6])\n",
    "embedding_sum(input, offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인스턴스 생성하기\n",
    "-----------------\n",
    "\n",
    "AG_NEWS 데이터셋에는 4 종류의 레이블이 달려 있으며, 따라서 클래스의 개수도 4개 입니다.\n",
    "\n",
    "   1 : World (세계)\n",
    "   2 : Sports (스포츠)\n",
    "   3 : Business (경제)\n",
    "   4 : Sci/Tec (과학/기술)\n",
    "\n",
    "어휘집의 크기(Vocab size)는 어휘집(vocab)의 길이와 같습니다 (여기에는\n",
    "각각의 단어와 ngrame이 모두 포함됩니다). 클래스의 개수는 레이블의 종류\n",
    "수와 같으며, AG_NEWS의 경우에는 4개 입니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 생성을 위한 함수들\n",
    "-----------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 원소의 길이가 다를 수 있으므로, 데이터 배치와 오프셋을 생성하기\n",
    "위한 사용자 함수 generate_batch()를 사용하려 합니다. 이 함수는\n",
    "``torch.utils.data.DataLoader`` 의 ``collate_fn`` 인자로 넘겨줍니다.\n",
    "\n",
    "``collate_fn`` 의 입력은 그 크기가 batch_size인 텐서들의 리스트이며,\n",
    "``collate_fn`` 은 이들을 미니배치로 묶는 역할을 합니다. 여러분이\n",
    "주의해야 할 점은, ``collate_fn`` 를 선언할 때 최상위 레벨에서 정의해야\n",
    "한다는 점입니다. 그래야 이 함수를 각각의 워커에서 사용할 수 있음이\n",
    "보장됩니다.\n",
    "\n",
    "원본 데이터 배치 입력의 텍스트 원소들은 리스트 형태이며, 이들을 하나의\n",
    "텐서가 되도록 이어 붙인 것이 ``nn.EmbeddingBag`` 의 입력이 됩니다.\n",
    "오프셋은 텍스트의 경계를 나타내는 텐서이며, 각 원소가 텍스트 텐서의\n",
    "어느 인덱스에서 시작하는지를 나타냅니다. 레이블은 각 텍스트 원소의\n",
    "레이블을 담고 있는 텐서입니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>collate_fn (callable, optional)</b>: merges a list of samples to form a\n",
    "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "        map-style dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum은 dim 차원의 요소들의 누적 합계를 반환합니다.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 학습하고 결과를 평가하는 함수 정의하기\n",
    "---------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 사용자라면\n",
    "`torch.utils.data.DataLoader`를 활용하는 것을 추천합니다. 또한 이를 사용하면 데이터를 쉽게 병렬적으로\n",
    "읽어올 수 있습니다 (이에 대한 튜토리얼은 `이 문서 <https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html>`\n",
    "를 참고하시기 바랍니다). 우리는 여기서 ``DataLoader`` 를 이용하여\n",
    "AG_NEWS 데이터셋을 읽어오고, 이를 모델로 넘겨 학습과 검증을 진행합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "    # Train the model\n",
    "    # 모델을 학습합니다\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    model.to(device)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # 학습율을 조절합니다\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋을 분할하고 모델 수행하기\n",
    "---------------------------------\n",
    "\n",
    "원본 AG_NEWS에는 검증용 데이터가 포함되어 있지 않기 때문에, 우리는 학습\n",
    "데이터를 학습 및 검증 데이터로 분할하려 합니다. 이때 데이터를 분할하는\n",
    "비율은 0.95(학습)와 0.05(검증) 입니다. 우리는 여기서 PyTorch의\n",
    "핵심 라이브러리 중 하나인\n",
    "`torch.utils.data.dataset.random_split`함수를 사용합니다.\n",
    "\n",
    "`CrossEntropyLoss`기준(criterion)은 각 클래스에 대해 nn.LogSoftmax()와 nn.NLLLoss()를\n",
    "합쳐 놓은 방식입니다.\n",
    "`SGD` optimizer는 확률적 경사 하강법를 구현해놓은 것입니다. 처음의 학습율은\n",
    "4.0으로 두었습니다. 매 에폭을 진행하면서 학습율을 조절할 때는\n",
    "`StepLR`을 사용합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 14 seconds\n",
      "\tLoss: 0.0262(train)\t|\tAcc: 84.7%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 91.0%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 14 seconds\n",
      "\tLoss: 0.0119(train)\t|\tAcc: 93.7%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 91.4%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 16 seconds\n",
      "\tLoss: 0.0069(train)\t|\tAcc: 96.4%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 91.2%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 14 seconds\n",
      "\tLoss: 0.0039(train)\t|\tAcc: 98.1%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 91.4%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 15 seconds\n",
      "\tLoss: 0.0023(train)\t|\tAcc: 98.9%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 91.5%(valid)\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 데이터로 모델 평가하기\n",
    "---------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0003(test)\t|\tAcc: 88.9%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 데이터셋을 통한 결과를 확인합니다...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의의 뉴스로 평가하기\n",
    "----------------------\n",
    "\n",
    "현재까지 구한 최고의 모델로 골프 뉴스를 테스트해보려 합니다. 레이블에\n",
    "대한 정보는\n",
    "`여기에 <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__\n",
    "나와 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "vocab = train_dataset.get_vocab()\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
