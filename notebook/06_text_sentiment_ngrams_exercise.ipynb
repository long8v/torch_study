{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to exercise 06\n",
    "1) 한국어 뉴스 데이터를 모아봅시다<br>\n",
    "2) 한국어 뉴스 데이터셋을 `torchtext.datasets.text_classification`처럼 작동하도록 전처리해봅시다<br>\n",
    "3) 모델을 무사히 돌려봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 한국어 뉴스 데이터를 모아봅시다\n",
    "Daum에서 섹션별로 뉴스를 긁어와 봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crawling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from downloads import *\n",
    "import requests\n",
    "import time\n",
    "import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = download(\"https://media.daum.net\")\n",
    "dom = BeautifulSoup(url.text,\"lxml\")\n",
    "lists = [_[\"href\"] for _ in dom.select(\".link_gnb\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/society\n",
      "/politics\n",
      "/economic\n",
      "/foreign\n"
     ]
    }
   ],
   "source": [
    "for section in lists[1:5]:\n",
    "    try:\n",
    "        os.mkdir(\"./scraping/\")\n",
    "    except:\n",
    "        None\n",
    "    try:\n",
    "        os.mkdir(\"./scraping/\"+section)\n",
    "        print(str(section)+\" folder is created.\")\n",
    "    except:\n",
    "        None\n",
    "    print(section)\n",
    "    URL = download(\"https://media.daum.net\"+section)\n",
    "    DOM = BeautifulSoup(URL.text,\"lxml\")\n",
    "    artcl_list = [_ for _ in DOM.select(\".tit_thumb a,.item_relate a\") if _.has_attr(\"href\")]\n",
    "    for _ in artcl_list:\n",
    "        DOM = BeautifulSoup(download(_[\"href\"]).text,\"lxml\")\n",
    "        try:\n",
    "            title = DOM.select_one(\"h3.tit_view\").text\n",
    "            summary = \" \".join([_.text for _ in DOM.select(\"strong.summary_view\") if _.has_attr(\"text\")])\n",
    "            contents = \" \".join([_.text for _ in DOM.select(\"div#harmonyContainer p\")])\n",
    "            link = _[\"href\"]\n",
    "            code = link.split(\"v/\")[-1]\n",
    "            with open(\"./scraping/\"+section+'/'+str(URL.url.split(\"net/\")[-1])+code+\".txt\",\"w\",encoding=\"UTF8\") as f:\n",
    "                f.write(title)\n",
    "                f.write(summary)\n",
    "                f.write(contents)\n",
    "                f.close()\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "저장된 txt 파일을 섹션을 key로 갖는 딕셔너리에 넣읍시다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file):\n",
    "    return open(file).readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = defaultdict(list)\n",
    "for folder in glob('scraping/*'):\n",
    "    for file in glob(folder+'/*'):\n",
    "        subject[folder.split('/')[1]].append(read_txt(file).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_number = dict(enumerate(list(subject.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_number_inverse = {value:key for key, value in subject_number.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = {subject_number_inverse[key]:value for key, value in subject.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngrams를 이용하여 데이터 불러오기\n",
    "\n",
    "Bag of ngrams 피쳐는 지역(local) 단어 순서에 대한 부분적인 정보를 포착하기 위해 적용합니다.\n",
    "실제 상황에서는 bi-gram이나 tri-gram은 단 하나의 단어를 이용하는 것보다 더 많은 이익을 주기 때문에 적용됩니다.\n",
    "예를 들면 다음과 같습니다.\n",
    "\n",
    "   \"load data with ngrams\"\n",
    "   Bi-grams 결과: \"load data\", \"data with\", \"with ngrams\"\n",
    "   Tri-grams 결과: \"load data with\", \"data with ngrams\"\n",
    "\n",
    "``TextClassification`` 데이터셋은 ngrams method을 지원합니다. ngrams을 2로 설정하면,\n",
    "데이터셋 안의 예제 텍스트는 각각의(single) 단어들에 bi-grams 문자열이 더해진 리스트가 될 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)  한국어 뉴스 데이터셋을 전처리해봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AGNEWS` 데이터셋에 맞게 전처리를 하기 위해 이와 같은 과정이 필요하다\n",
    "1) 각 단어들을 tokenize함 <br>\n",
    "2) uni-gram + bi-gram 형태의 리스트로 만들어줌<br>\n",
    "3) \\<sos> 토큰과 \\<eos> 토큰을 추가해줌<br> \n",
    "4) 토큰을 숫자로 바꿔 줌.<br>\n",
    "5) tensor로 바꿔 줌.<br>\n",
    "이를 쉽게 하기 위하여 `torchtext.data`의 `Field`를 사용해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*class* <b>torchtext.data.Field</b>(sequential=True, use_vocab=True, init_token=None, eos_token=None, fix_length=None, dtype=torch.int64, preprocessing=None, postprocessing=None, lower=False, tokenize=None, tokenizer_language='en', include_lengths=False, batch_first=False, pad_token='<pad>', unk_token='<unk>', pad_first=False, truncate_first=False, stop_words=None, is_target=False)\n",
    "\n",
    "어떻게 text processing을 할 것인지에 대해 나타냄. `Vocab`이란 object를 가지고 있어 token들과 token의 숫자표현을 담고 있음. `Field` object는 또한 데이터타입이 어떻게 numericalize되는지에 대한 방법도 담고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "from torchtext.data.utils import ngrams_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "field = Field(init_token = '<sos>',\n",
    "              eos_token = '<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ' '.join([' '.join(s) \n",
    "                   for sub in subject.values() \n",
    "                   for s in sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '나는', '강북', '멋쟁이', '안녕 나는', '나는 강북', '강북 멋쟁이']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngrams_iterator를 하면 uni-gram + bi-gram으로 나옴\n",
    "list(ngrams_iterator('안녕 나는 강북 멋쟁이'.split(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "corpus_split = corpus.split()\n",
    "corpus_ngram = list(ngrams_iterator(corpus_split, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "field.build_vocab([corpus_ngram])\n",
    "vocab = field.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['트럼프']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4074"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['트럼프 대통령']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['트럼프 회장']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 예제의 형태처럼 category와 텐서화된 문장을 리스트로 넣어줌\n",
    "subject_numerical = []\n",
    "for key, articles in subject.items():\n",
    "    for article in articles:\n",
    "        article_ngram = list(ngrams_iterator(article, 2))\n",
    "        numeric_v = torch.Tensor([vocab[token] for token in article_ngram]).type(torch.int64)\n",
    "        subject_numerical.append((key, numeric_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " tensor([ 2432,  1067,  1722,   485, 24260, 28391, 43915,  1573, 35213,    15,\n",
       "          1436,  1067,   568, 10775,  1692,  8170,   978,   433,  3472,  4150,\n",
       "          2564,  1722,   279,   216, 24278,  7234, 16343,   896,  3595,     7,\n",
       "           433,  3472,  1573,   145,    19, 15387,  2930,   313,   976,   279,\n",
       "           238,  7661,  2564,  1722,   216, 24264,  1142,   391,   184,  5726,\n",
       "         17400,   672,   145, 15633,  1819,    15,  8593,  2848,    13,  2517,\n",
       "          2803, 28063,   564, 30855, 15607,   986,  3190,  9610,   718,  2874,\n",
       "            37, 22222,   216, 24281,   213,  8609, 39143,  1072,  6558,   156,\n",
       "            45, 15577,   366,   534,  3020,  1069,   218,    15,  3149,   139,\n",
       "          3508,   534,  3520, 25311,   366,  5676,  3336,   534,  3312,   623,\n",
       "          1428,  3510,  4199,  3117,  1023,  2774, 26931,    11,   145, 15684,\n",
       "          3040,  1427,  1861,  3094,   715,  3050,  3884,  3415,  3001, 45938,\n",
       "         15641,  3310,   834,  4232,  3171,   552,   247,   366,  3588,  3391,\n",
       "           640,   605, 38013, 15611, 10681,  1023, 42440, 15457, 31039, 33561,\n",
       "         17069,  7721, 33224, 29508,   121, 20598,    68,   238,   721, 20817,\n",
       "           238,   213,  1072,   271,   145,  3518,   733,  5547,  8874,  3623,\n",
       "           718, 37127,   733,  5547, 20591,   107,   145, 15461, 14875,   528,\n",
       "         27570, 38785, 24894,  6963, 22081,  6945,  6004, 40481,   868,  2012,\n",
       "          2370,     8,  1486, 45689,    68, 46803,    11,   482,   474,  6769,\n",
       "         32587,   111, 16482,   856, 23016,   156, 32035,  2781,   640,   733,\n",
       "         46447, 43262,   145, 15483, 31028,   366,  1858, 17205,  2404,    31,\n",
       "         20671, 32591,  3709,  9317, 42324,  4186,    75,   229,   238, 39813,\n",
       "          6306, 38015,  7516,    45, 15716,   839, 34535, 41553,   366, 45217,\n",
       "         23437,  2256, 43446,    37,  9618, 36988,   782, 32471,  2068,  9366,\n",
       "          7907,    66, 25560,  7533, 10618,   145,   718, 37339,   221,  7392,\n",
       "           216,  1002, 29129, 18326,    51, 15637,   221,  1921,  3283,  1582,\n",
       "             4,  1260,   216,  1002,   213,  1072, 23521, 15615,  3688,  3616,\n",
       "           216,  6069,    65,   868,  1427,   986,  3187,   216, 40316,    11,\n",
       "           273,   145, 41359,     4,  1436,   238,   568,  1121,   156, 15740,\n",
       "           188,   485,  3876,    65,    38,  3479,    52,    58, 32517,   537,\n",
       "           718, 45940, 15459,  2932,  1421,  1133,  1146, 46555,   733,     6,\n",
       "          2589,  2269,   840,  1145,    75,  1824,   420,  7516,   145,  5044,\n",
       "           806,  9082,  4249,   720,   485,    85,    14,  1164,  3867,  3995,\n",
       "          9205, 10951, 15635,   756,   709,   430,  1879,    31,    64,   976,\n",
       "           238,  1377,  1899,  1430,  1914,  3855, 44968,    76,  8932,  6536,\n",
       "           279,  8949,  1921,   478,  1067,   669,   335,   498,  1251,  1428,\n",
       "            34, 24336,  6980,  9023,     6,    52,    58,   238,  9860, 44682,\n",
       "          1411, 46484,     7,  5980,  2177, 11863, 32422, 18868, 35305, 24261,\n",
       "         28392, 43916, 45226, 35214,  2357,  3519,  3473,  9862, 10776, 17901,\n",
       "         31203, 20806,  4126,  8422, 45228,  2565, 18867, 33399, 35313, 24279,\n",
       "         26922, 16344, 42589, 34925, 37851,  4126, 32421, 10911,  4933, 13589,\n",
       "         15388,  2931,  3122,  2775,  8592, 10910,  7662,  2565,  5645, 35312,\n",
       "         24265,  9880,  5843, 10386, 19181, 17401, 34677, 14841, 15634,  1820,\n",
       "         10925,  8594,  2849,  3741,  2518, 21051, 28064, 38248, 30856, 15608,\n",
       "          2866, 27796, 38508,  4149,  2875, 19509, 22223, 35314, 24282, 29142,\n",
       "         33535, 39144, 32925, 23519,  3898, 36658, 15578,  3478,  7236,  3021,\n",
       "          3493,  9280,  4155,  7280,  3009,  3509,  3134, 33463, 25312, 32509,\n",
       "          5677,  3337,  7237,  3313, 24318,  8449,  3511,  4200,  3118,  3116,\n",
       "         20639, 26932, 11056, 14843, 15685,  3041,  3477,  3015,  3095,  4136,\n",
       "          3051,  3885,  3416, 24804, 45939, 15642,  3311,  3498,  4233,  3172,\n",
       "          8489,  5389,  8442,  3589,  3392,  7433, 17787, 38014, 15612, 44178,\n",
       "         26677, 42441, 15458, 31040, 33562, 17070, 29284, 33225, 29509, 17950,\n",
       "         20599, 24122, 45211, 46018, 20818, 45207,  1380,  8507, 23617, 14845,\n",
       "         33402,  4935, 18316, 34945, 35310, 45212, 37128,  4935, 18315, 20592,\n",
       "         45387, 14839, 15462, 14876, 25322, 27571, 38786, 24895, 25839, 22082,\n",
       "         25757, 20641, 40482, 38780, 30393, 11008,  3409, 37613, 45690, 24125,\n",
       "         46804, 45906, 34283, 27688, 24733, 32588, 17125, 16483, 36779, 23017,\n",
       "         40724, 32036, 20695, 27936, 14873, 46448, 43263, 14840, 15484, 31029,\n",
       "         32510, 25090, 17206,  4272, 23022, 20672, 32592,  9320, 36994, 42325,\n",
       "         11016,   662,  6567, 45208, 39814, 22186, 38016, 28302, 36665, 15717,\n",
       "         33541, 34536, 41554, 32512, 45218, 23438, 40588, 43447, 19529, 38554,\n",
       "         36989, 23897, 32472, 32721, 37245, 30166, 45054, 25561,  7534, 43911,\n",
       "         14850, 45213, 37340, 45692, 27794,  1091, 24280, 29130, 18327, 19600,\n",
       "         15638,  4185,  3189,  7729, 45974,  3665,  1261,  1091,  1318,  1380,\n",
       "         32926, 23522, 15616,  3689,  3617,  8951,  6070, 10141,  9648,  8440,\n",
       "          2865,  3188, 35317, 40317, 45887, 24488, 14848, 41360, 36135, 33406,\n",
       "          2356, 39819, 38019, 40718, 15741,  5384,  3622,  3877,  3946,  3609,\n",
       "          3480,   524, 21903, 32518,  3233, 45216, 45941, 15460,  2933, 32069,\n",
       "         39123, 40206, 46556,  4936,  2957,  2590,  3926,  3531,  9921,  8464,\n",
       "         23551, 37863, 28301, 14842, 15654,  7667,  9083,  4250,  4193,  3621,\n",
       "          1315,  2899,  3999,  3868, 10356, 36390, 45416, 15636,  5619, 10349,\n",
       "          4050, 25994,   461, 36555,  2776,  4148,  3255,  3121,  3497,  3153,\n",
       "         40002, 44969, 16397, 35210,  6537,  8590, 35309, 27795,  7842,  8423,\n",
       "          8698,  9919, 10089, 17892,  8448, 24084, 24337,  6981,  9024, 23948,\n",
       "           524, 21916, 45210, 39815, 44683, 31202, 46485, 37787,  5981]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_numerical[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(subject_numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VainallaDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    def get_vocab(self):\n",
    "        return field.vocab\n",
    "    def get_labels(self):\n",
    "        return list(subject.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = VainallaDataset(train)\n",
    "test_ds = VainallaDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 모델을 무사히 돌려봅시다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의하기\n",
    "\n",
    "우리의 모델은\n",
    "`EmbeddingBag`레이어와 선형 레이어로 구성됩니다 (아래 그림 참고).\n",
    "``nn.EmbeddingBag``는 임베딩들로 구성된 '가방'의 평균을 계산합니다.\n",
    "이때 텍스트(text)의 각 원소는 그 길이가 다를 수 있습니다. 텍스트의\n",
    "길이는 오프셋(offset)에 저장되어 있으므로 여기서 ``nn.EmbeddingBag``\n",
    "에 패딩을 사용할 필요는 없습니다.\n",
    "\n",
    "덧붙여서, ``nn.EmbeddingBag`` 은 임베딩의 평균을 즉시 계산하기 때문에,\n",
    "텐서들의 시퀀스를 처리할 때 성능 및 메모리 효율성 측면에서의 장점도\n",
    "갖고 있습니다.\n",
    "\n",
    "![](../_static/img/text_sentiment_ngrams_model.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인스턴스 생성하기\n",
    "크롤링한 뉴스 데이터셋에는 4 종류의 레이블이 달려 있으며, 따라서 클래스의 개수도 4개 입니다.\n",
    "\n",
    "어휘집의 크기(Vocab size)는 어휘집(vocab)의 길이와 같습니다 (여기에는\n",
    "각각의 단어와 ngram이 모두 포함됩니다)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(train_ds.get_vocab()) # 이를 하기 위해 Dataset을 정의할 때 get_vocab이란 메소드를 만들어 줌\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_ds.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47293"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds.get_vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 생성을 위한 함수들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 원소의 길이가 다를 수 있으므로, 데이터 배치와 오프셋을 생성하기\n",
    "위한 사용자 함수 generate_batch()를 사용하려 합니다. 이 함수는\n",
    "``torch.utils.data.DataLoader`` 의 ``collate_fn`` 인자로 넘겨줍니다.\n",
    "\n",
    "``collate_fn`` 의 입력은 그 크기가 batch_size인 텐서들의 리스트이며,\n",
    "``collate_fn`` 은 이들을 미니배치로 묶는 역할을 합니다. 여러분이\n",
    "주의해야 할 점은, ``collate_fn`` 를 선언할 때 최상위 레벨에서 정의해야\n",
    "한다는 점입니다. 그래야 이 함수를 각각의 워커에서 사용할 수 있음이\n",
    "보장됩니다.\n",
    "\n",
    "원본 데이터 배치 입력의 텍스트 원소들은 리스트 형태이며, 이들을 하나의\n",
    "텐서가 되도록 이어 붙인 것이 ``nn.EmbeddingBag`` 의 입력이 됩니다.\n",
    "오프셋은 <b>텍스트의 경계를 나타내는 텐서</b>이며, 각 원소가 텍스트 텐서의\n",
    "어느 인덱스에서 시작하는지를 나타냅니다. 레이블은 각 텍스트 원소의\n",
    "레이블을 담고 있는 텐서입니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>collate_fn (callable, optional)</b>: <br>merges a list of samples to form a\n",
    "        mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "        map-style dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### offsets 예제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4510, -2.5885,  3.0341],\n",
       "        [-0.4592, -0.2884,  1.3650],\n",
       "        [-0.2143, -1.9077, -1.3087]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') \n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([0,1,2,4,5,4,3,2,9])\n",
    "offsets = torch.LongTensor([0,5,8]) # 0, 1~5, 6~8이 한개의 문서다\n",
    "embedding_sum(input, offsets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text] # [0, 첫번째 텍스트의 길이, 두번째 텍스트의 길이 ...]\n",
    "    # torch.Tensor.cumsum은 dim 차원의 요소들의 누적 합계를 반환합니다.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # [0, 첫번째 텍스트가 끝나는 인덱스, ...모델을]\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델을 학습하고 결과를 평가하는 함수 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 사용자라면\n",
    "`torch.utils.data.DataLoader`를 활용하는 것을 추천합니다. 또한 이를 사용하면 데이터를 쉽게 병렬적으로\n",
    "읽어올 수 있습니다 (이에 대한 튜토리얼은 `이 문서 <https://tutorials.pytorch.kr/beginner/data_loading_tutorial.html>`\n",
    "를 참고하시기 바랍니다). 우리는 여기서 ``DataLoader`` 를 이용하여\n",
    "AG_NEWS 데이터셋을 읽어오고, 이를 모델로 넘겨 학습과 검증을 진행합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "    # Train the model\n",
    "    # 모델을 학습합니다\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # 학습율을 조절합니다\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋을 분할하고 모델 수행하기\n",
    "\n",
    "\n",
    "원본 AG_NEWS에는 검증용 데이터가 포함되어 있지 않기 때문에, 우리는 학습\n",
    "데이터를 학습 및 검증 데이터로 분할하려 합니다. 이때 데이터를 분할하는\n",
    "비율은 0.95(학습)와 0.05(검증) 입니다. 우리는 여기서 PyTorch의\n",
    "핵심 라이브러리 중 하나인\n",
    "`torch.utils.data.dataset.random_split`함수를 사용합니다.\n",
    "\n",
    "`CrossEntropyLoss`기준(criterion)은 각 클래스에 대해 nn.LogSoftmax()와 nn.NLLLoss()를\n",
    "합쳐 놓은 방식입니다.\n",
    "`SGD` optimizer는 확률적 경사 하강법를 구현해놓은 것입니다. 처음의 학습율은\n",
    "4.0으로 두었습니다. 매 에폭을 진행하면서 학습율을 조절할 때는\n",
    "`StepLR`을 사용합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0784(train)\t|\tAcc: 46.6%(train)\n",
      "\tLoss: 0.4779(valid)\t|\tAcc: 80.0%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0713(train)\t|\tAcc: 83.0%(train)\n",
      "\tLoss: 0.5170(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0666(train)\t|\tAcc: 80.7%(train)\n",
      "\tLoss: 0.4836(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0635(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4774(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 25  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0627(train)\t|\tAcc: 85.2%(train)\n",
      "\tLoss: 0.4747(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 30  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0603(train)\t|\tAcc: 86.4%(train)\n",
      "\tLoss: 0.4803(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 35  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0599(train)\t|\tAcc: 86.4%(train)\n",
      "\tLoss: 0.4795(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 40  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0601(train)\t|\tAcc: 86.4%(train)\n",
      "\tLoss: 0.4785(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 45  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0601(train)\t|\tAcc: 86.4%(train)\n",
      "\tLoss: 0.4777(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 50  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0586(train)\t|\tAcc: 88.6%(train)\n",
      "\tLoss: 0.4771(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 55  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0599(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4772(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 60  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0611(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4770(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 65  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0586(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4771(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 70  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0590(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4771(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 75  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0600(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4771(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 80  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0585(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4771(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 85  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0588(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4770(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 90  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0610(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4770(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 95  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0589(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4770(valid)\t|\tAcc: 40.0%(valid)\n",
      "Epoch: 100  | time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0595(train)\t|\tAcc: 87.5%(train)\n",
      "\tLoss: 0.4770(valid)\t|\tAcc: 40.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 100\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_ds) * 0.95)\n",
    "sub_train_, sub_valid_ = random_split(train_ds, [train_len, len(train_ds) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "        print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "        print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가 데이터로 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0764(test)\t|\tAcc: 58.1%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_ds)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 데이터셋을 통한 결과를 확인합니다...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 임의의 뉴스로 평가하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_label = subject_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'society', 1: 'politics', 2: 'economic', 3: 'foreign'}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = lambda e: e.split()\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_string(string, model, train_ds):\n",
    "    vocab = train_ds.get_vocab()\n",
    "    model = model.to(\"cpu\")\n",
    "    print(\"This is a %s news\" %news_label[predict(string, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a politics news\n"
     ]
    }
   ],
   "source": [
    "predict_string('이건희', model, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a foreign news\n"
     ]
    }
   ],
   "source": [
    "predict_string('환매', model, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a economic news\n"
     ]
    }
   ],
   "source": [
    "predict_string('이슬람', model, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a society news\n"
     ]
    }
   ],
   "source": [
    "predict_string('대통령', model, train_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long36v",
   "language": "python",
   "name": "long36v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
