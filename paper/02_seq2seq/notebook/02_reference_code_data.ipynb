{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Torch Study "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2월 2~3주차 : Sequence to Sequence Learning with Neural Networks\n",
    "논문 디테일 구현해보기 \n",
    "- seq2seq 자체 (v)\n",
    "- Encoder와 Decoder를 연결시켜주는 부분 (v)\n",
    "- greedy search decoder(=predict) (v)\n",
    "- Beam search decoder (x)\n",
    "- packed_padded_sequence (v)\n",
    "- Batch로 넣어줄 때, sequence length 별로 sort해서 넣어주는 것 (v)\n",
    "- most frequent 단어만 사용하고 나머지는 [UNK] 처리함 (v)\n",
    "- LSTM weight uniform 초기화 (v)\n",
    "- loss ⇒ $1/|S| * \\sum_{(T,S)\\in \\mathbf{S}}logP(T|S)$\n",
    "- gradient clipping 과 halving learning rate (x)\n",
    "- BLEU 계산 (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, Iterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.7.1', '0.8.1')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set the random seeds for deterministic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create the tokenizers. A tokenizer is used to turn a string containing a sentence into a list of individual tokens that make up that string, e.g. \"good morning!\" becomes [\"good\", \"morning\", \"!\"]. We'll start talking about the sentences being a sequence of tokens from now, instead of saying they're a sequence of words. What's the difference? Well, \"good\" and \"morning\" are both words and tokens, but \"!\" is a token, not a word. \n",
    "\n",
    "spaCy has model for each language (\"de_core_news_sm\" for German and \"en_core_web_sm\" for English) which need to be loaded so we can access the tokenizer of each model. \n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "We load the models as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the tokenizer functions. These can be passed to torchtext and will take in the sentence as a string and return the sentence as a list of tokens.\n",
    "\n",
    "In the paper we are implementing, they find it beneficial to reverse the order of the input which they believe \"introduces many short term dependencies in the data that make the optimization problem much easier\". We copy this by reversing the German sentence after it has been transformed into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchtext's `Field`s handle how data should be processed. All of the possible arguments are detailed [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L61). \n",
    "\n",
    "We set the `tokenize` argument to the correct tokenization function for each, with German being the `SRC` (source) field and English being the `TRG` (target) field. The field also appends the \"start of sequence\" and \"end of sequence\" tokens via the `init_token` and `eos_token` arguments, and converts all words to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download and load the train, validation and test data. \n",
    "\n",
    "The dataset we'll be using is the [Multi30k dataset](https://github.com/multi30k/dataset). This is a dataset with ~30,000 parallel English, German and French sentences, each with ~12 words per sentence. \n",
    "\n",
    "`exts` specifies which languages to use as the source and target (source goes first) and `fields` specifies which field to use for the source and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check that we've loaded the right number of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out an example, making sure the source sentence is reversed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The period is at the beginning of the German (src) sentence, so it looks like the sentence has been correctly reversed.\n",
    "\n",
    "Next, we'll build the *vocabulary* for the source and target languages. The vocabulary is used to associate each unique token with an index (an integer). The vocabularies of the source and target languages are distinct.\n",
    "\n",
    "Using the `min_freq` argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an `<unk>` (unknown) token.\n",
    "\n",
    "It is important to note that our vocabulary should only be built from the training set and not the validation/test set. This prevents \"information leakage\" into our model, giving us artifically inflated validation/test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## most frequent인 n개만 사용하는 거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_MOST_FREQ = 4000\n",
    "TRG_MOST_FREQ = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, max_size = SRC_MOST_FREQ)\n",
    "TRG.build_vocab(train_data, max_size = TRG_MOST_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 4004\n",
      "Unique tokens in target (en) vocabulary: 2004\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.stoi[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4005, 2004)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SRC.vocab.stoi), len(TRG.vocab.stoi) # special token 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of preparing the data is to create the iterators. These can be iterated on to return a batch of data which will have a `src` attribute (the PyTorch tensors containing a batch of numericalized source sentences) and a `trg` attribute (the PyTorch tensors containing a batch of numericalized target sentences). Numericalized is just a fancy way of saying they have been converted from a sequence of readable tokens to a sequence of corresponding indexes, using the vocabulary. \n",
    "\n",
    "We also need to define a `torch.device`. This is used to tell torchText to put the tensors on the GPU or not. We use the `torch.cuda.is_available()` function, which will return `True` if a GPU is detected on our computer. We pass this `device` to the iterator.\n",
    "\n",
    "When we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences. Luckily, torchText iterators handle this for us! \n",
    "\n",
    "We use a `BucketIterator` instead of the standard `Iterator` as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비슷한 길이 끼리 묶어주기\n",
    "- Iterator에서 sort_key를 `src`의 길이로 줌\n",
    "- sort=True 안하면 sort가 안된다\n",
    "- BucketIterator와 Iterator는 다르다 (https://stackoverflow.com/questions/49367871/concept-of-bucketing-in-seq2seq-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .to('cuda')는 가장 마지막(모델에 넣을때) 해주는게 가장 효율적임\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# device = 'cpu'\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BucketIterator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort_within_batch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
      "        [   4,    4,    4,  ...,    4,    4,    4],\n",
      "        [  21,   75,  119,  ...,  214, 1034,   45],\n",
      "        ...,\n",
      "        [2911,   67,   13,  ...,   53, 1104,    0],\n",
      "        [  18,    8,    5,  ...,   73,   65,    0],\n",
      "        [   3,    3,    3,  ...,    3,    3,    3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for _ in train_iterator:\n",
    "    print(_.src)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 128]) torch.Size([11, 128])\n",
      "torch.Size([13, 128]) torch.Size([13, 128])\n",
      "torch.Size([15, 128]) torch.Size([10, 128])\n",
      "torch.Size([17, 128]) torch.Size([14, 128])\n",
      "torch.Size([16, 128]) torch.Size([16, 128])\n",
      "torch.Size([12, 128]) torch.Size([13, 128])\n",
      "torch.Size([19, 128]) torch.Size([20, 128])\n",
      "torch.Size([13, 128]) torch.Size([15, 128])\n",
      "torch.Size([17, 128]) torch.Size([19, 128])\n",
      "torch.Size([21, 128]) torch.Size([25, 128])\n",
      "torch.Size([15, 128]) torch.Size([15, 128])\n",
      "torch.Size([21, 128]) torch.Size([27, 128])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_iterator):\n",
    "    print(batch.src.shape, batch.trg.shape)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator\n",
    "**class Iterator**(object):\n",
    "    \"\"\"Defines an iterator that loads batches of data from a Dataset.\n",
    "\n",
    "    Attributes:\n",
    "        dataset: The Dataset object to load Examples from.\n",
    "        batch_size: Batch size.\n",
    "        batch_size_fn: Function of three arguments (new example to add, current\n",
    "            count of examples in the batch, and current effective batch size)\n",
    "            that returns the new effective batch size resulting from adding\n",
    "            that example to a batch. This is useful for dynamic batching, where\n",
    "            this function would add to the current effective batch size the\n",
    "            number of tokens in the new example.\n",
    "        sort_key: A key to use for sorting examples in order to batch together\n",
    "            examples with similar lengths and minimize padding. The sort_key\n",
    "            provided to the Iterator constructor overrides the sort_key\n",
    "            attribute of the Dataset, or defers to it if None.\n",
    "        train: Whether the iterator represents a train set.\n",
    "        repeat: Whether to repeat the iterator for multiple epochs. Default: False.\n",
    "        shuffle: Whether to shuffle examples between epochs.\n",
    "        sort: Whether to sort examples according to self.sort_key.\n",
    "            Note that shuffle and sort default to train and (not train).\n",
    "        sort_within_batch: Whether to sort (in descending order according to\n",
    "            self.sort_key) within each batch. If None, defaults to self.sort.\n",
    "            If self.sort is True and this is False, the batch is left in the\n",
    "            original (ascending) sorted order.\n",
    "        device (str or `torch.device`): A string or instance of `torch.device`\n",
    "            specifying which device the Variables are going to be created on.\n",
    "            If left as default, the tensors will be created on cpu. Default: None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sort_key는 전체 데이터 정렬\n",
    "- sort는 sort_key로 정렬할지 말지\n",
    "- sort_within_batch는 배치 내에서 sort_key로 descending하게 정렬할지 말지. sort가 True면 이것도 True이고, sort가 True고 sort_with_batch가 False이면 ascending하게 됨 -> `packed_padded_sequence`때문에 True로 줘야하고, 굳이 False로 두는 상황은 모르겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator = Iterator(train_data, \n",
    "                          batch_size = BATCH_SIZE, \n",
    "                          sort_key=lambda e: len(e.src),\n",
    "                          sort=True,\n",
    "                          sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Example\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort_key를 주면 shuffle이 안되는 거 아닐까?\n",
    "아니다! sort를 하더라도 매 epoch별로 shuffle를 하고 sort를 하면 데이터는 달라질 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '하이'), (1, '방가'), (1, '룰루'), (1, 'i will kill you'), (2, 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(1, '하이'), (1, '방가'), (1, '룰루'), (1, 'i will kill you'), (2, 3)]\n",
    "sorted(data, key = lambda e: e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '방가'), (1, '룰루'), (1, '하이'), (1, 'i will kill you'), (2, 3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(data)\n",
    "sorted(data, key = lambda e: e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 128]) torch.Size([13, 128])\n",
      "torch.Size([8, 128]) torch.Size([13, 128])\n",
      "torch.Size([8, 128]) torch.Size([15, 128])\n",
      "torch.Size([8, 128]) torch.Size([14, 128])\n",
      "torch.Size([8, 128]) torch.Size([14, 128])\n",
      "torch.Size([8, 128]) torch.Size([16, 128])\n",
      "torch.Size([9, 128]) torch.Size([16, 128])\n",
      "torch.Size([9, 128]) torch.Size([14, 128])\n",
      "torch.Size([9, 128]) torch.Size([15, 128])\n",
      "torch.Size([9, 128]) torch.Size([16, 128])\n",
      "torch.Size([9, 128]) torch.Size([15, 128])\n",
      "torch.Size([9, 128]) torch.Size([13, 128])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_iterator):\n",
    "    print(batch.src.shape, batch.trg.shape)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = map(lambda x: Iterator(x,\n",
    "                                       batch_size = BATCH_SIZE,\n",
    "                                       sort_key = lambda e: len(e.src),\n",
    "                                       sort = True, device = device),\n",
    "                    [train_data, valid_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 128]) torch.Size([13, 128])\n",
      "torch.Size([8, 128]) torch.Size([13, 128])\n",
      "torch.Size([8, 128]) torch.Size([15, 128])\n",
      "torch.Size([8, 128]) torch.Size([14, 128])\n",
      "torch.Size([8, 128]) torch.Size([14, 128])\n",
      "torch.Size([8, 128]) torch.Size([16, 128])\n",
      "torch.Size([9, 128]) torch.Size([16, 128])\n",
      "torch.Size([9, 128]) torch.Size([14, 128])\n",
      "torch.Size([9, 128]) torch.Size([15, 128])\n",
      "torch.Size([9, 128]) torch.Size([16, 128])\n",
      "torch.Size([9, 128]) torch.Size([15, 128])\n",
      "torch.Size([9, 128]) torch.Size([13, 128])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_iterator):\n",
    "    print(batch.src.shape, batch.trg.shape)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model\n",
    "\n",
    "We'll be building our model in three parts. The encoder, the decoder and a seq2seq model that encapsulates the encoder and decoder and will provide a way to interface with each.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "First, the encoder, a 2 layer LSTM. The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut this down to 2-layers. The concept of multi-layer RNNs is easy to expand from 2 to 4 layers. \n",
    "\n",
    "multi-layer RNN은 input sentence인 $X$은 RNN의 첫번째 (가장 바닥의) 레이어에서 임베딩(H=\\{h_1, h_2, ..., h_T\\})된 뒤에 , 그 레이어의 output이 그 위의 RNN의 input으로 들어갑니다. 그러므로, 각 레이어를 위첨자로 표현하면, first layer의 hidden state는 아래와 같습니다:\n",
    "\n",
    "$$h_t^1 = \\text{EncoderRNN}^1(e(x_t), h_{t-1}^1)$$\n",
    "\n",
    "The hidden states in the second layer are given by:\n",
    "\n",
    "$$h_t^2 = \\text{EncoderRNN}^2(h_t^1, h_{t-1}^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "? embedding dim과 rnn의 hidden dim은 같아야 겠네<br>\n",
    "-> 상관없음 stacked RNN에서 모든 RNN이 크기가 같을 필욘 없음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a multi-layer RNN also means we'll also need an initial hidden state as input per layer, $h_0^l$, and we will also output a context vector per layer, $z^l$.\n",
    "\n",
    "Without going into too much detail about LSTMs (see [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) blog post to learn more about them), all we need to know is that they're a type of RNN which instead of just taking in a hidden state and returning a new hidden state per time-step, also take in and return a *cell state*, $c_t$, per time-step.\n",
    "\n",
    "$$\\begin{align*}\n",
    "h_t &= \\text{RNN}(e(x_t), h_{t-1})\\\\\n",
    "(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\n",
    "\\end{align*}$$\n",
    "\n",
    "We can just think of $c_t$ as another type of hidden state. Similar to $h_0^l$, $c_0^l$ will be initialized to a tensor of all zeros. Also, our context vector will now be both the final hidden state and the final cell state, i.e. $z^l = (h_T^l, c_T^l)$.\n",
    "\n",
    "Extending our multi-layer equations to LSTMs, we get:\n",
    "\n",
    "$$\\begin{align*}\n",
    "(h_t^1, c_t^1) &= \\text{EncoderLSTM}^1(e(x_t), (h_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(h_t^2, c_t^2) &= \\text{EncoderLSTM}^2(h_t^1, (h_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "\n",
    "Note how only our hidden state from the first layer is passed as input to the second layer, and not the cell state.\n",
    "\n",
    "So our encoder looks something like this: \n",
    "\n",
    "![](assets/seq2seq2.png)\n",
    "\n",
    "We create this in code by making an `Encoder` module, which requires we inherit from `torch.nn.Module` and use the `super().__init__()` as some boilerplate code. The encoder takes the following arguments:\n",
    "- `input_dim` is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n",
    "- `emb_dim` is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with `emb_dim` dimensions. \n",
    "- `hid_dim` is the dimensionality of the hidden and cell states.\n",
    "- `n_layers` is the number of layers in the RNN.\n",
    "- `dropout` is the amount of dropout to use. This is a regularization parameter to prevent overfitting. Check out [this](https://www.coursera.org/lecture/deep-neural-network/understanding-dropout-YaGbR) for more details about dropout.\n",
    "\n",
    "We aren't going to discuss the embedding layer in detail during these tutorials. All we need to know is that there is a step before the words - technically, the indexes of the words - are passed into the RNN, where the words are transformed into vectors. To read more about word embeddings, check these articles: [1](https://monkeylearn.com/blog/word-embeddings-transform-text-numbers/), [2](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html), [3](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/), [4](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/). \n",
    "\n",
    "The embedding layer is created using `nn.Embedding`, the LSTM with `nn.LSTM` and a dropout layer with `nn.Dropout`. Check the PyTorch [documentation](https://pytorch.org/docs/stable/nn.html) for more about these.\n",
    "\n",
    "\n",
    "One thing to note is that the `dropout` argument to the LSTM is how much dropout to apply between the layers of a multi-layer RNN, 즉 multi-layer RNN에서 layer $l$의 hidden state output 과 layer $l+1$의 input hidden state 사이에서 적용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `forward` method, we pass in the source sentence, $X$, which is converted into dense vectors using the `embedding` layer, and then dropout is applied. These embeddings are then passed into the RNN. 우리가 RNN에게 시퀀스 전체를 넣어줘도, 이것은 자동으로 모든 시퀀스의 hidden state에 대한 recurrent 계산을 해줄 것입니다! 우리가 initial hidden 이나 cell state를 RNN에게 넣어주지 않아도 된다는 점을 알아주세요(참고 : [documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM)), 만약 RNN에 넣어준 hidden/cell state가 없다면, 이것은 자동으로 제로 텐서로 넣어줄 것입니다.\n",
    "\n",
    "The RNN returns: `outputs` (각 time-step에서 최상단 layer의 hidden state), `hidden` (the final hidden state for each layer, $h_T$, stacked on top of each other) and `cell` (the final cell state for each layer, $c_T$, stacked on top of each other).\n",
    "\n",
    "우리는 context vector를 만들기 위해 오직 마지막 hidden과 cell state를 필요로 하기 때문에, `forward`는 오직 `hidden`과 `cell`만을 return 합니다.\n",
    "\n",
    "The sizes of each of the tensors is left as comments in the code. In this implementation `n_directions` will always be 1, however note that bidirectional RNNs (covered in tutorial 3) will have `n_directions` as 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nn.LSTM`\n",
    "### **Inputs:**  input, (h_0, c_0)\n",
    "\n",
    "**input** of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "\n",
    "**h_0** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "**c_0** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch.\n",
    "\n",
    "If (h_0, c_0) is not provided, both h_0 and c_0 default to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **outputs:** output, (h_n, c_n)\n",
    "**output** of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.\n",
    "\n",
    "For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.\n",
    "\n",
    "**h_n** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "\n",
    "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n.\n",
    "\n",
    "**c_n** of shape (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [1., 2.]],\n",
       "\n",
       "        [[3., 4.],\n",
       "         [3., 4.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([[1,2,1,2], [3,4,3,4]]).view(2, 2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Next, we'll build our decoder, which will also be a 2-layer (4 in the paper) LSTM.\n",
    "\n",
    "![](assets/seq2seq3.png)\n",
    "\n",
    "`Decoder` 클래스는 decoding의 한 스텝만 진행합니다(즉 time-step당 하나의 토큰만 output으로 내놓습니다). 첫번째 레이어는 그 전 time-step의 hidden, cell state($(s_{t-1}^1, c_{t-1}^1)$)를 받고, 이것을 LSTM에 현재의 임베딩된 토큰$y_t$과 함께 넣습니다. 그 다음의 layer는 그 아래 레이어의 hidden state($s_t^{l-1}$)와 그들 레이어의 이전 hidden state와 cell state $(s_{t-1}^l, c_{t-1}^l)$.를 사용할 것입니다. 이 것은 인코더에 있는 식과 비슷한 식으로 표현할 수 있습니다.\n",
    "\n",
    "$$\\begin{align*}\n",
    "(s_t^1, c_t^1) = \\text{DecoderLSTM}^1(d(y_t), (s_{t-1}^1, c_{t-1}^1))\\\\\n",
    "(s_t^2, c_t^2) = \\text{DecoderLSTM}^2(s_t^1, (s_{t-1}^2, c_{t-1}^2))\n",
    "\\end{align*}$$\n",
    "\n",
    "우리의 initial hidden, cell state는 context vector임을 기억하세요. context vector는 같은 레이어의 마지막 hidden, cell state입니다. 즉, $(s_0^l,c_0^l)=z^l=(h_T^l,c_T^l)$.\n",
    "\n",
    "We then pass the hidden state from the top layer of the RNN, $s_t^L$, through a linear layer, $f$, to make a prediction of what the next token in the target (output) sequence should be, $\\hat{y}_{t+1}$. \n",
    "\n",
    "$$\\hat{y}_{t+1} = f(s_t^L)$$\n",
    "\n",
    "이제 target의 vocab size와 같은 `output_dim`가 추가된 것을 제외하고는 argumetns와 초기화는 `Encoder` 클래스와 비슷합니다. \n",
    "그리고 추가적으로 `Linear` 레이어가 있어서, 가장 위의 layer의 hidden state를 통해 예측을 하게 됩니다.\n",
    "\n",
    "`forward` method 내에서는,  input token 한 배치, 과거의 hidden state와 과거의 cell state를 받습니다. 우리가 한번에 하나의 토큰만 디코딩하기 때문에, input token의 시퀀스 길이는 언제나 1입니다. 우리는 이 때문에 sequence length dimension을 추가하기 위해 `unsqueeze`를 사용합니다. 그리고, 인코더와 유사하게, 우리는 임베딩 레이어를 통과 시키고 dropout을 적용시킵니다. 임베딩된 토큰 한 배치는 이 전의 hidden, cell state와 함께 RNN을 통과합니다. 이것은 `output` (hidden state from the top layer of the RNN), 새로운 `hidden` state (one for each layer, stacked on top of each other), 새로운 `cell` state (also one per layer, stacked on top of each other)를 만듭니다. 그리고 나서 우리는 `output` (after getting rid of the sentence length dimension)을 linear layer에 넣고 우리의 `prediction`을 얻습니다. 우리는 `prediction`, 새 `hidden` state, 새 `cell` state를 반환합니다.\n",
    "\n",
    "**Note**: as we always have a sequence length of 1, we could use `nn.LSTMCell`, instead of `nn.LSTM`, as it is designed to handle a batch of inputs that aren't necessarily in a sequence. `nn.LSTMCell` is just a single cell and `nn.LSTM` is a wrapper around potentially multiple cells. Using the `nn.LSTMCell` in this case would mean we don't have to `unsqueeze` to add a fake sequence length dimension, but we would need one `nn.LSTMCell` per layer in the decoder and to ensure each `nn.LSTMCell` receives the correct initial hidden state from the encoder. All of this makes the code less concise - hence the decision to stick with the regular `nn.LSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "For the final part of the implemenetation, we'll implement the seq2seq model. This will handle: \n",
    "- receiving the input/source sentence\n",
    "- using the encoder to produce the context vectors \n",
    "- using the decoder to produce the predicted output/target sentence\n",
    "\n",
    "Our full model will look like this:\n",
    "\n",
    "![](assets/seq2seq4.png)\n",
    "\n",
    "The `Seq2Seq` model takes in an `Encoder`, `Decoder`, and a `device` (used to place tensors on the GPU, if it exists).\n",
    "\n",
    "For this implementation, we have to ensure that the number of layers and the hidden (and cell) dimensions are equal in the `Encoder` and `Decoder`. This is not always the case, we do not necessarily need the same number of layers or the same hidden dimension sizes in a sequence-to-sequence model. However, if we did something like having a different number of layers then we would need to make decisions about how this is handled. For example, if our encoder has 2 layers and our decoder only has 1, how is this handled? Do we average the two context vectors output by the decoder? Do we pass both through a linear layer? Do we only use the context vector from the highest layer? Etc.\n",
    "\n",
    "우리의 `forward` method는 source sentence, target sentence, teacher-forcing ratio를 받습니다. teacher forcing ratio는 training할 때 사용됩니다. decoding시에 각 time-step에서 우리는 과거 decoded된 토큰에서 다음 토큰$\\hat{y}_{t+1}=f(s_t^L)$을 예측합니다. teahcer forcing ratio의 확률로 우리는 실제 ground truth 다음 토큰을 다음 time-step에서 인풋 토큰으로 사용할 것이고 (1 - teacher_forcing_ratio)의 확률로 모델이 예측한 토큰을 사용할 것입니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward` method에서 가장 먼저 해야할 것은 우리의 모든 prediction $\\hat{Y}$ 을 저장할 `outputs`텐서를 만드는 것입니다. \n",
    "그리고 나서 source sentence를 encoder에 넣고 우리의 마지막 hidden, cell state를 받습니다.\n",
    "The first input to the decoder is the start of sequence (`<sos>`) token. As our `trg` tensor already has the `<sos>` token appended (all the way back when we defined the `init_token` in our `TRG` field) we get our $y_1$ by slicing into it. 우리는 target 문장이 얼마나 길어야 할지(`max_len`)알기 때문에, 그만큼 루프를 돌면 됩니다. 마지막 토큰은 `<eos>`토큰 바로 전까지의 토큰입니다 - 절대로 `<eos>` 토큰이 디코더에 들어가면 안됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iteration을 돌 때마다, 우리는\n",
    "- input과 과거의 hidden, cell state($y_t, s_{t-1}, c_{t-1}$)를 디코더에 넣습니다\n",
    "- 예측값, 다음 hidden, cell state($\\hat{y}_{t+1}, s_{t}, c_{t}$)를 디코더로 부터 받습니다\n",
    "- 우리의 예측값인  $\\hat{y}_{t+1}$/`output`를 우리의 예측을 위한 텐서 $\\hat{Y}$/`outputs`에 넣습니다\n",
    "- \"teacher force\"를 할지 말지 정합니다 \n",
    "  - 만약 한다면, 다음 `input`은 시퀀스 내 ground-truth 다음 토큰 $y_{t+1}$/`trg[t]`이 될 것입니다\n",
    "  - 아니라면, 우리의 다음 `input`은 시퀀스 내에서 예측된 다음 토큰 $\\hat{y}_{t+1}$/`top1`이고 이것은 ouput tensor에 `argrmax`를 함으로서 얻어집니다\n",
    "  \n",
    "우리의 모든 예측을 만들면, predictions으로 채워진 tensor$\\hat{Y}$/`outputs`를 리턴합니다\n",
    "\n",
    "**Note**: 우리의 디코더 loop는 0이 아니고 1에서 시작합니다. 이는 우리의 `ouputs`의 0번째 원소는 0으로 남아있음을 의미합니다. 그래서 우리의 `trg`와 `outputs`는 아래와 같이 생겼을 것입니다\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "Later on when we calculate the loss, we cut off the first element of each tensor to get:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, src, max_len):\n",
    "        with torch.no_grad():\n",
    "            outputs = []\n",
    "            print(src.shape)\n",
    "#             src = src.transpose(1, 0) # (B, T) -> (T, B)\n",
    "            bs = src.shape[1]\n",
    "    #         print(src.shape)\n",
    "            hidden, cell = self.encoder(src)\n",
    "            input = torch.Tensor([2] * bs).long().to(self.device) # <SOS> token\n",
    "    #         |input| torch.Size([1]) |hidden| torch.Size([2, 1, 512]) |cell| torch.Size([2, 1, 512])\n",
    "\n",
    "            for _ in range(max_len): \n",
    "                output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "    #         |output| torch.Size([1, 10858]) |hidden| torch.Size([2, 1, 512]) |cell| torch.Size([2, 1, 512])\n",
    "                top1 = output.argmax(1)\n",
    "                input = top1\n",
    "                outputs.append(int(top1.data))\n",
    "                if top1.data == 3:\n",
    "                    break\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Seq2Seq Model\n",
    "\n",
    "Now we have our model implemented, we can begin training it. \n",
    "\n",
    "First, we'll initialize our model. As mentioned before, the input and output dimensions are defined by the size of the vocabulary. The embedding dimesions and dropout for the encoder and decoder can be different, but the number of layers and the size of the hidden/cell states must be the same. \n",
    "\n",
    "We then define the encoder, decoder and then our Seq2Seq model, which we place on the `device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is initializing the weights of our model. In the paper they state they initialize all weights from a uniform distribution between -0.08 and +0.08, i.e. $\\mathcal{U}(-0.08, 0.08)$.\n",
    "\n",
    "We initialize weights in PyTorch by creating a function which we `apply` to our model. When using `apply`, the `init_weights` function will be called on every module and sub-module within our model. For each module we loop through all of the parameters and sample them from a uniform distribution with `nn.init.uniform_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(4004, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2004, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=2004, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function that will calculate the number of trainable parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 9,922,516 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our optimizer, which we use to update our parameters in the training loop. Check out [this](http://ruder.io/optimizing-gradient-descent/) post for information about different optimizers. Here, we'll use Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimizer + halving learning rate every half epoch\n",
    "train을 7.5 epoch하고, learning rate도 5를 넘는 half epoch마다 lr을 halving 해줘야 하기 때문에 train 중간일 때 에폭을 세자<br>\n",
    "-> 아 몰랑 Adam 쓸래"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR, MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 learning rate : 0.7\n",
      "Epoch 1 learning rate : 0.7\n",
      "Epoch 2 learning rate : 0.7\n",
      "Epoch 3 learning rate : 0.7\n",
      "Epoch 4 learning rate : 0.7\n",
      "Epoch 5 learning rate : 0.35\n",
      "Epoch 6 learning rate : 0.175\n",
      "Epoch 7 learning rate : 0.0875\n",
      "Epoch 8 learning rate : 0.04375\n",
      "Epoch 9 learning rate : 0.021875\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.7)\n",
    "scheduler = MultiStepLR(optimizer, milestones=list(np.arange(5, 10, 0.5)), gamma=0.5)\n",
    "for _ in range(10):\n",
    "    print(f'Epoch {_} learning rate : {scheduler.get_last_lr()[0]}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our loss function. The `CrossEntropyLoss` function calculates both the log softmax as well as the negative log-likelihood of our predictions. \n",
    "\n",
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define our training loop. \n",
    "\n",
    "First, we'll set the model into \"training mode\" with `model.train()`. This will turn on dropout (and batch normalization, which we aren't using) and then iterate through our data iterator.\n",
    "\n",
    "As stated before, our decoder loop starts at 1, not 0. This means the 0th element of our `outputs` tensor remains all zeros. So our `trg` and `outputs` look something like:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [<sos>, &y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [0, &\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "Here, when we calculate the loss, we cut off the first element of each tensor to get:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} = [&y_1, y_2, y_3, <eos>]\\\\\n",
    "\\text{outputs} = [&\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, <eos>]\n",
    "\\end{align*}$$\n",
    "\n",
    "At each iteration:\n",
    "- batch로 부터 $X$와 $Y$를 받습니다\n",
    "- 마지막 배치로 부터 계산된 gradient를 0으로 초기화합니다\n",
    "- source와 target을 모델에 넣고 output $\\hat{Y}$를 받습니다 \n",
    "- loss function이 2D input과 1D target에서만 작동하므로 우리는 .view로 각각을 flatten해줍니다\n",
    "- 앞서 언급한 대로 ouput의 첫번째 컬럼을 슬라이싱해서 제거해줍니다\n",
    "- `loss.backward()`로 gradient를 계산해줍니다\n",
    "- gradient exploding을 방지하기 위해 clipping을 해줍니다(RNN에서 흔한 이슈)\n",
    "- optimizer step을 통해 모델의 파라미터들을 업데이트해줍니다\n",
    "- loss를 전체 런닝에 합쳐줍니다\n",
    "\n",
    "그러면 우리는 모든 배치에 대한 평균적인 loss를 구할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    check_half = len(iterator) // 2 \n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value.\n",
    "\n",
    "We must remember to set the model to evaluation mode with `model.eval()`. This will turn off dropout (and batch normalization, if used).\n",
    "\n",
    "We use the `with torch.no_grad()` block to ensure no gradients are calculated within the block. This reduces memory consumption and speeds things up. \n",
    "\n",
    "The iteration loop is similar (without the parameter updates), however we must ensure we turn teacher forcing off for evaluation. This will cause the model to only use it's own predictions to make further predictions within a sentence, which mirrors how it would be used in deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that we'll use to tell us how long an epoch takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally start training our model!\n",
    "\n",
    "At each epoch, we'll be checking if our model has achieved the best validation loss so far. If it has, we'll update our best validation loss and save the parameters of our model (called `state_dict` in PyTorch). Then, when we come to test our model, we'll use the saved parameters used to achieve the best validation loss. \n",
    "\n",
    "We'll be printing out both the loss and the perplexity at each epoch. It is easier to see a change in perplexity than a change in loss as the numbers are much bigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 11s \n",
      "\tTrain Loss: 7.235 | Train PPL: 1387.087\n",
      "\t Val. Loss: 6.395 |  Val. PPL: 598.848\n",
      "Epoch: 02 | Time: 0m 11s \n",
      "\tTrain Loss: 5.551 | Train PPL: 257.434\n",
      "\t Val. Loss: 5.319 |  Val. PPL: 204.119\n",
      "Epoch: 03 | Time: 0m 12s \n",
      "\tTrain Loss: 5.068 | Train PPL: 158.907\n",
      "\t Val. Loss: 5.168 |  Val. PPL: 175.635\n",
      "Epoch: 04 | Time: 0m 11s \n",
      "\tTrain Loss: 4.923 | Train PPL: 137.456\n",
      "\t Val. Loss: 5.187 |  Val. PPL: 178.903\n",
      "Epoch: 05 | Time: 0m 10s \n",
      "\tTrain Loss: 4.878 | Train PPL: 131.375\n",
      "\t Val. Loss: 5.147 |  Val. PPL: 171.952\n",
      "Epoch: 06 | Time: 0m 11s \n",
      "\tTrain Loss: 4.849 | Train PPL: 127.640\n",
      "\t Val. Loss: 4.997 |  Val. PPL: 147.926\n",
      "Epoch: 07 | Time: 0m 11s \n",
      "\tTrain Loss: 4.793 | Train PPL: 120.699\n",
      "\t Val. Loss: 4.896 |  Val. PPL: 133.767\n",
      "Epoch: 08 | Time: 0m 11s \n",
      "\tTrain Loss: 4.757 | Train PPL: 116.374\n",
      "\t Val. Loss: 4.850 |  Val. PPL: 127.755\n",
      "Epoch: 09 | Time: 0m 11s \n",
      "\tTrain Loss: 4.734 | Train PPL: 113.789\n",
      "\t Val. Loss: 4.827 |  Val. PPL: 124.880\n",
      "Epoch: 10 | Time: 0m 11s \n",
      "\tTrain Loss: 4.718 | Train PPL: 111.918\n",
      "\t Val. Loss: 4.823 |  Val. PPL: 124.331\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.7)\n",
    "# scheduler = MultiStepLR(optimizer, milestones=list(np.arange(5, 10, 0.5)), gamma=0.5)\n",
    "scheduler = optim.Adam(model.parameters())\n",
    "\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    scheduler.step()\n",
    "#     print(scheduler.get_lr())\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s ')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the parameters (`state_dict`) that gave our model the best validation loss and run it the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 4.799 | Test PPL: 121.337 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook we'll implement a model that achieves improved test perplexity, but only uses a single layer in the encoder and the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ field 구현 ==============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from glob import glob\n",
    "sys.path.append('/home/long8v/torch_study/paper/01_CNN/source/')\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchtext field, build_vocab 구현\n",
    "이유 : `Example.fromlist`가 안먹네요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/long8v/torch_study/paper/file'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(f'{filepath}/multi30k/train*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_path = glob(f'{filepath}/multi30k/train*de')[0]\n",
    "en_path = glob(f'{filepath}/multi30k/train*en')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(en_path) as f:\n",
    "    en = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(de_path) as f:\n",
    "    de = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:    \n",
    "    def build_vocabs(self, sentence_list):\n",
    "        self.stoi_dict = defaultdict(lambda: 1) \n",
    "        self.stoi_dict['<PAD>'] = 0\n",
    "        self.stoi_dict['<UNK>'] = 1\n",
    "        self.stoi_dict['<SOS>'] = 2\n",
    "        self.stoi_dict['<EOS>'] = 3\n",
    "        _index = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in sentence:\n",
    "                if word in self.stoi_dict:\n",
    "                    pass\n",
    "                else:\n",
    "                    self.stoi_dict[word] = _index\n",
    "                    _index += 1\n",
    "        self.itos_dict = {v:k for k, v in self.stoi_dict.items()}\n",
    "        \n",
    "    def stoi(self, token_list):\n",
    "        return [self.stoi_dict[word] for word in token_list]\n",
    "\n",
    "    def itos(self, indices):\n",
    "        return \" \".join([self.itos_dict[int(index)] for index in indices if self.itos_dict[index] != '<PAD>'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class field:\n",
    "    def __init__(self, tokenize = lambda e: e.split(), init_token = '<SOS>', \n",
    "                 eos_token = '<EOS>', lower = False, reverse = False):\n",
    "        self.tokenize = tokenize\n",
    "        self.init_token = init_token\n",
    "        self.eos_token = eos_token\n",
    "        self.lower = lower\n",
    "        self.reverse = reverse\n",
    "        self.vocab = None\n",
    "    \n",
    "    def build_vocab(self, data):\n",
    "        self.vocab = Vocab()\n",
    "        self.vocab.build_vocabs(self.get_processed_datalist(data))\n",
    "\n",
    "    def get_processed_data(self, data):\n",
    "        if self.lower:\n",
    "            data = data.lower()\n",
    "        tokenized_data = self.tokenize(data)\n",
    "        if self.reverse:\n",
    "            tokenized_data = tokenized_data[::-1]\n",
    "        if self.init_token:\n",
    "            tokenized_data = [self.init_token] + tokenized_data\n",
    "        if self.eos_token:\n",
    "            tokenized_data = tokenized_data + [self.eos_token]\n",
    "        return tokenized_data\n",
    "    \n",
    "    def get_processed_datalist(self, datalist):\n",
    "        return [self.get_processed_data(data) for data in datalist]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = field(tokenize_de, '<SOS>', '<EOS>', False, reverse=True)\n",
    "TRG = field(tokenize_en, '<SOS>', '<EOS>', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(de)\n",
    "TRG.build_vocab(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.get_processed_datalist(['hi hello', 'hi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### namedtuple for `.src`, `.trg` access "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple  \n",
    "      \n",
    "# Declaring namedtuple()   \n",
    "Student = namedtuple('Student',['name','age','DOB'])   \n",
    "      \n",
    "# Adding values   \n",
    "S = Student('Nandini','19','2541997')   \n",
    "      \n",
    "# Access using index   \n",
    "print (\"The Student age using index is : \",end =\"\")   \n",
    "print (S[1])   \n",
    "      \n",
    "# Access using name    \n",
    "print (\"The Student name using keyname is : \",end =\"\")   \n",
    "print (S.name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple  \n",
    "\n",
    "class seq2seqDataset(Dataset):\n",
    "    def __init__(self, src, trg = None, field = None, device = 'cpu'):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.data_source = {'src':src, 'trg':trg}\n",
    "        self.field = field\n",
    "        self.device = device\n",
    "        self.named_tuple = namedtuple('data', ['src', 'trg'])\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.trg is None:\n",
    "            return self.getitem('src', idx)\n",
    "        return self.named_tuple(self.getitem('src', idx), self.getitem('trg', idx))\n",
    "    \n",
    "    def getitem(self, field_name, idx):\n",
    "        data = self.data_source[field_name][idx]\n",
    "        field = self.field[field_name]\n",
    "        tokenize_data = field.get_processed_data(data)\n",
    "        return torch.Tensor(self.field[field_name].vocab.stoi(tokenize_data)).long().to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 3]) tensor([   2, 7210,    3])\n",
      "tensor([2, 1, 3]) tensor([   2, 7210,    3])\n"
     ]
    }
   ],
   "source": [
    "ds = seq2seqDataset('dd', 'ee', {'src':SRC, 'trg':TRG})\n",
    "for _ in ds:\n",
    "    print(_.src, _.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (src, trg) = zip(*batch)\n",
    "    src_pad = pad_sequence(src, batch_first=False, padding_value=0)\n",
    "    trg_pad = pad_sequence(trg, batch_first=False, padding_value=0)\n",
    "    return src_pad, trg_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(zip(de, en), key=lambda e: len(e[0]))\n",
    "de, en = list(zip(*sorted_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seq2seqDataset(de, en, field = {'src':SRC, 'trg':TRG}, device=device)\n",
    "dl = DataLoader(dataset, batch_size=10, collate_fn=pad_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n",
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [    4,     4,     4,     4,     4,     4,     4,     4,     4,     4],\n",
      "        [    3, 12911, 12911,     5,     5,     5,  7562,     5,     5,     5],\n",
      "        [    0,     3,     3,  4154,  9043,   296,    77,    61,   307,  1458],\n",
      "        [    0,     0,     0,    41,   134,  6648,     3,   578,    12,   273],\n",
      "        [    0,     0,     0,    30,    76,    30,     0,   331,   402,  2622],\n",
      "        [    0,     0,     0,     3,     3,     3,     0,     3,    17,    30],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     3,     3]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for _ in dl:\n",
    "    print(_[0].data.shape)\n",
    "    print(_[0].data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pack_padded_sequence\n",
    "Packs a Tensor containing padded sequences of variable length.\n",
    "\n",
    "input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected.\n",
    "\n",
    "For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export.<br>\n",
    "**input (Tensor)** – padded batch of variable length sequences.\n",
    "\n",
    "**lengths (Tensor)** – list of sequences lengths of each batch element.\n",
    "\n",
    "**batch_first (bool, optional)** – if True, the input is expected in B x T x * format.\n",
    "\n",
    "**enforce_sorted (bool, optional)** – if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, the input will get sorted unconditionally. Default: True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "def pack_pad_collate(batch):\n",
    "    (src, trg) = zip(*batch)\n",
    "    src_len = torch.Tensor([len(s) for s in src])\n",
    "    trg_len = torch.Tensor([len(t) for t in trg])\n",
    "    src_pad = pad_sequence(src, batch_first=False, padding_value=0)\n",
    "    trg_pad = pad_sequence(trg, batch_first=False, padding_value=0)\n",
    "    src_pack = pack_padded_sequence(src_pad, lengths=src_len, batch_first=False, enforce_sorted=False)\n",
    "    trg_pack = pack_padded_sequence(trg_pad, lengths=trg_len, batch_first=False, enforce_sorted=False)\n",
    "    return src_pack, trg_pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sorting by length of de\n",
    "...길이 정렬을 하려면 string일 때 길이가 아니라, token화한 후 길이를 재야함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(zip(de, en), key=lambda e: len(e[0]))\n",
    "de, en = list(zip(*sorted_list)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seq2seqDataset(de, en, field = {'src':SRC, 'trg':TRG}, device=device)\n",
    "dl = DataLoader(dataset, batch_size=10, collate_fn=pack_pad_collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.stoi_dict['<EOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60])\n",
      "tensor([    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "            4,     4,     4,     4,     4,     4,     4,     4,     4,     4,\n",
      "            5,     5,     5,     5,     5,     5,  7562, 12911, 12911,     3,\n",
      "          307,  1458,  4154,  9043,   296,    61,    77,     3,     3,    12,\n",
      "          273,    41,   134,  6648,   578,     3,   402,  2622,    30,    76,\n",
      "           30,   331,    17,    30,     3,     3,     3,     3,     3,     3],\n",
      "       device='cuda:0')\n",
      "tensor([10, 10, 10,  9,  7,  6,  6,  2])\n",
      "tensor([8, 9, 3, 4, 5, 7, 6, 1, 2, 0], device='cuda:0')\n",
      "tensor([9, 7, 8, 2, 3, 4, 6, 5, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for _ in dl:\n",
    "    print(_[0].data.shape)\n",
    "    print(_[0].data)\n",
    "    print(_[0].batch_sizes)\n",
    "    print(_[0].sorted_indices)\n",
    "    print(_[0].unsorted_indices)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_de = [\"Welt\", \"Welt\", \"Welt WeltWelt\"]\n",
    "# i am learning deep learning and machine learning. how many time does \"learning\" repeat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = seq2seqDataset(test_de, field={'src': SRC}, device=device)\n",
    "test_dl = DataLoader(test_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 11750,     3]], device='cuda:0')\n",
      "tensor([[    2, 11750,     3]], device='cuda:0')\n",
      "tensor([[    2,     1, 11750,     3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for _ in test_dl:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welt', 'Welt', 'Welt WeltWelt']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<EOS>', 'Welt', '<SOS>']\n",
      "['<EOS>', 'Welt', '<SOS>']\n",
      "['<EOS>', 'Welt', '<UNK>', '<SOS>']\n"
     ]
    }
   ],
   "source": [
    "for _ in test_dl:\n",
    "    print([SRC.vocab.itos_dict[int(idx)] for idx in _.data[0]][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 128])\n",
      "torch.Size([13, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/long8v/anaconda3/envs/long36v/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for _ in train_iterator:\n",
    "    print(_.src.shape)\n",
    "    print(_.trg.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in test_dl:\n",
    "#     print(TRG.vocab.itos(model.predict(batch, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in dl:\n",
    "#     src, trg = batch\n",
    "#     src, _= pad_packed_sequence(src)\n",
    "#     trg, _ = pad_packed_sequence(trg)\n",
    "#     output = model.predict(src, 10)\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long36v",
   "language": "python",
   "name": "long36v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
