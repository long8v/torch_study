## ğŸ¤— Result
WIP

## ğŸ¤” Paper review
**1) PPT í•œ ì¥ ë¶„ëŸ‰ìœ¼ë¡œ ììœ ë¡­ê²Œ ë…¼ë¬¸ ì •ë¦¬ ë’¤ ì´ë¯¸ì§€ë¡œ ì²¨ë¶€**
![Untitled (1)](https://user-images.githubusercontent.com/46675408/108203501-f15d6f00-7165-11eb-9c68-8da61962b002.png)

**2) (ìŠ¬ë™ìœ¼ë¡œ ì´ë¯¸ í† ë¡ ì„ í–ˆì§€ë§Œ ê·¸ë˜ë„) ì´í•´ê°€ ì•ˆ ê°€ëŠ” ë¶€ë¶„, ì´í•´ê°€ ì•ˆ ê°€ëŠ” ì´ìœ (ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**

= ìš°ë¦¬ì˜ objective functionëŠ” ? 

ê³„ì† ë§í•˜ê³  ìˆëŠ” perplexityê°€ ëª¨ë“  tì‹œì ì˜ softmax ê²°ê³¼ê°’ì¸ logSoftmax lossë¥¼ ë‹¤ ë”í•œê²ƒì´ê² ì§€? â†’ ã…‡ã…‡ ë§ë‹¤<br>
= ë…¼ë¬¸ì—ì„œ ì„ë² ë”© ì‹œê°í™”í•œ ê²ƒ : Thus the deep LSTM uses 8000 real number to represnt a sentenceì—ì„œ 8000ì€ 1000(=hidden cell dim) * 4(=num layers of LSTM)  * 2(=hidden, cell state) ì¸ê±´ ì•Œê² ëŠ”ë° PCAí• ë•Œ ê·¸ëƒ¥ concatí–ˆìœ¼ë ¤ë‚˜ ì•„ë‹˜ (1000 by 4 by 2) ë¥¼ PCA? í›„ìì¼ë“¯?

= SMTì—ì„œ rescore?

SMT ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¬¼ì„ ê°œì„ ì‹œí‚¤ê¸° ìœ„í•´ SMTì—ì„œ ë½‘íŒ 1000ê°œì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¬ì •ë ¬í•˜ëŠ” ê²ƒ

Finally, we used the LSTM to rescore the publicly available 1000-best lists of the SMT baseline on
the same task [29]. By doing so, we obtained a BLEU score of 36.5, which improves the baseline by
3.2 BLEU points and is close to the previous best published result on this task (which is 37.0 [9]).

**3) ì¬ë°Œì—ˆë˜ ë¶€ë¶„**

= reversed : ë‚˜ì¤‘ì— RNN seq2seqì“°ëŠ” ê²ƒ ìˆìŒ ì‹¤í—˜í•´ë´ì•¼ê² ë‹¤ ì‹¶ì—ˆìŒ! 

= PCAí•´ë´¤ë”ë‹ˆ ë‹¨ì–´ëŠ” ê±°ì˜ ë¹„ìŠ·í•œë° ì˜ë¯¸ëŠ” ë°˜ëŒ€ì¸ ê²ƒì´ ë¬¶ì¸ ê²ƒ. ê²°ê³¼ê°€ ê½¤ ë†€ë¼ì›Œì„œ ì²´ë¦¬í”¼í‚¹ì¸ì§€ ì•„ë‹Œì§€ ê¼­ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë´ì•¼ê² ë‹¤

= softmax êµ¬í•˜ëŠ”ë°ë§Œ 4ê°œì˜ GPUë¥¼ ì“´ ê²ƒ...ã…‹ã…‹â†’ ê·¸ëŸ¼ ìš”ì¦˜ì€ ë‹¤ hierachial softmax ì“°ëŠ”ê±´ê°€?

= sizeê°€ 1ì¸ beam search ì¦‰ greedy search ì„±ëŠ¥ì´ ë‚˜ì˜ì§€ ì•Šì•˜ë˜ ê²ƒ. ë‚´ê°€ seq2seq + greedy searchë¥¼ ì¼ì„ ë• ê³„ì† í™•ë¥ ì´ ë†’ì€ ê²ƒ ê°™ì€ ë˜‘ê°™ì€ ë‹¨ì–´ë¥¼ ë°˜ë³µí•˜ë˜ë° ê·¸ëŸ°ê±´ BLEUì—ì„œ í¬ê²Œ penalty ë˜ì§€ ì•Šì•„ì„œì¼ê¹Œ? ì•„ë‹ˆë©´ ë‚´ê°€ íŠ¸ë ˆì´ë‹ì„ ì´ìƒí•˜ê²Œ ì‹œì¼œì„œ ê·¸ëŸ°ê±¸ê¹Œ 

= We found deep LSTMs to significantly outperform shallow LSTMs, where each additional layer reduced perplexity by nearly 10% â†’ ë” ê¹Šì€ê²Œ í•­ìƒ ì¢‹ì€ê±´ ì•„ë‹Œë° ì´ ê²½ìš°ì—” ê¹Šì€ê²Œ í›¨ì”¬ ì¢‹ì•˜ë‹¤ë„¤..ê·¸ëƒ¥ ì‹ ê¸°

**4) ë…¼ë¬¸ êµ¬í˜„ ì‹œ ì£¼ì˜í•´ì•¼í•  ê²ƒ ê°™ì€ ë¶€ë¶„(ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**

= most frequent ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” [UNK] ì²˜ë¦¬í•¨ â†’ ê²°êµ­ corpus í•œ ë°”í€´ ë‹¤ ë´ì•¼í•¨ã…ã…
We used 160,000 of the most frequent words for the source language
and 80,000 of the most frequent words for the target language. Every out-of-vocabulary word was
replaced with a special â€œUNKâ€ token.

= LSTM weight uniform ì´ˆê¸°í™”
We initialized all of the LSTMâ€™s parameters with the uniform distribution between -0.08
and 0.08

= beam search decoder 
We search for the most likely translation using a simple left-to-right beam search decoder

= exploding gradientë¥¼ í”¼í•˜ê¸° ìœ„í•˜ì—¬ L2 norm gradient clipping
Thus we enforced a hard constraint on the norm of the gradient by scaling it when its norm exceeded a threshold.

= ë¹„ìŠ·í•œ ê¸¸ì´ ì• ë“¤ë¼ë¦¬ ë¬¶ì–´ì¤˜ì•¼í•¨! 
To address this problem, we made sure
that all sentences in a minibatch are roughly of the same length, yielding a 2x speedup.

## ğŸ¤« ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê²Œ êµ¬í˜„í•œ ë¶€ë¶„
- dataset 
- optimizer
- halving learning rate every half epoch
- beam search ë¯¸êµ¬í˜„..

## ğŸ¤­ ë…¼ë¬¸ êµ¬í˜„í•˜ë©´ì„œ ë°°ìš´ ì  / ëŠë‚€ ì 
- [bucketingì´ ë­”ì§€](https://stackoverflow.com/questions/49367871/concept-of-bucketing-in-seq2seq-model)(bucketiteratorê°€ ë‹¨ìˆœíˆ ê¸¸ì´ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì£¼ëŠ” ê²ƒë¿ ì•„ë‹ˆë¼ bucketingì´ë¼ëŠ” ì—°ì‚°ê¹Œì§€ í•´ì¤€ë‹¤ëŠ” ì )
- `pack_padded_sequence`, `pad_packed_sequence` : ë°°ì¹˜ë¡œ ë¬¶ì„ ë•Œ zero-paddingì´ ìƒê¸°ê³  RNNì´ í•´ë‹¹ zero-paddingì„ êµ³ì´ ê±°ì¹˜ì§€ ì•Šê²Œ í•˜ëŠ” ê²ƒì´ [packing](https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html)
- `.to(device)`ëŠ” ëª¨ë¸ì— ë°ì´í„° ë¶€ì„ë•Œ ë„£ëŠ”ê²Œ ê°€ì¥ íš¨ìœ¨ì ì´ë‹¤
- torchtextì˜ `Field` êµ¬í˜„í•´ ë´„
- collectionsì˜ namedtuple ë§¤ìš° ìœ ìš©(DataLoader êµ¬ì„±í•  ë•Œ `.src` ì ‘ê·¼í•˜ë ¤ê³  ì‚¬ìš©í•¨) 
- Iteratorì˜ sort, sort_key, sort_within_batch argument
- torchì˜ `nn.LSTM`ì˜ input output shape. for ë¬¸ìœ¼ë¡œ hidden, cell ì•ˆë„£ì–´ ì¤˜ë„ ëª¨ë“  ì‹œí€€ìŠ¤ì— ëŒ€í•´ recurrent ê³„ì‚°ì„ í•´ì¤Œ
- multi-layered LSTMì˜ encoder-decoder ì—°ê²°í•˜ëŠ” cell, hiddenì´ ëª¨ë“  layerì—ì„œ ì—°ê²°ë˜ë„ë¡ êµ¬í˜„ë˜ì–´ ìˆë‹¤ëŠ” ì 
- seq2seqì˜ decoderë¥¼ í•™ìŠµí•  ë•Œì—ëŠ” `<eos>`í† í°ì´ ë“¤ì–´ê°€ë©´ ì•ˆë¨ 
- seq2seqì˜ encoderëŠ” `<sos>` í† í°ì„ ì•ˆ ë„£ì–´ë„ ë¨
- teacher forceì´ ì½”ë“œ ìƒ ì–´ë–»ê²Œ êµ¬í˜„ë˜ëŠ”ì§€
