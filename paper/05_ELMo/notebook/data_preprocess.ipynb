{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korpora.fetch('namuwikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = Korpora.load('namuwikitext')\n",
    "# with open('kor.p', 'wb') as f:\n",
    "#     pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kor.p', 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mecab\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "import sys\n",
    "sys.path.append('../source')\n",
    "from txt_cleaner.clean.master import MasterCleaner\n",
    "from txt_cleaner.utils import *\n",
    "from torch8text.data import Vocab, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## field 1: mecab 사용 field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = mecab.MeCab()\n",
    "\n",
    "def tokenize_pos(inp):\n",
    "    if type(inp) == str:\n",
    "        return pos.morphs(inp)\n",
    "    if type(inp) == list:\n",
    "        return [tokenize_pos(i) for i in inp]\n",
    "# pos.morphs(['안녕하세요'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['안녕', '하', '세요'], [['안녕', '하', '세요'], ['안녕', '?']])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_pos('안녕하세요'), tokenize_pos(['안녕하세요', '안녕?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수인님 cleaner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 1 dictionary is read from ../source/txt_cleaner/cleaner_config.json\n"
     ]
    }
   ],
   "source": [
    "config = json_reader('../source/txt_cleaner/cleaner_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum_space_count': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['minimum_space_count'] = 2\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요? 반갑습니다! 행복하세요'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner = MasterCleaner(config)\n",
    "cleaner.cleaning('안녕하세요? 반갑습니다! 행복하세요~**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['안녕', '하', '세요'], ['안녕']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_pos(['안녕하세요', '안녕'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_field = Field(tokenize = tokenize_pos, \n",
    "                 preprocessing = cleaner.cleaning,\n",
    "                    init_token = False,\n",
    "                    eos_token = False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '하', '세요', '룰루랄라']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [text for text in corpus.train.texts if cleaner.cleaning(text)]\n",
    "mecab_field.build_vocab(train)\n",
    "mecab_field.preprocess('안녕하세요 룰루랄라 ㅇㅇㄹ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## field 2:  chr-level field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner.cleaning('아')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = MasterCleaner({'minimum_space_count':0})\n",
    "chr_field = Field(tokenize = list, \n",
    "                 preprocessing = lambda e: cleaner.cleaning(e) if len(e) > 1 else e,\n",
    "                  init_token = False,\n",
    "                  eos_token = False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_field.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr_field.process('안녕하세요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰별로 한번, 토큰 내 캐릭터 레벨로 한번 데이터셋을 구성해야 함 \n",
    "-> 토큰에서 unk이면 어차피 chr level도 unk이기 때문에 {토큰 idx: 캐릭터 idx 리스트} 딕셔너리를 만들면 쉽지 않을까? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for key, value in mecab_field.vocab.stoi_dict.items():\n",
    "    print(value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>', '<PAD>', '<SOS>', '<EOS>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_field.vocab.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_dict = {key: chr_field.process(value) \n",
    "            if value not in mecab_field.vocab.special_tokens\n",
    "            else [0]\n",
    "            for key, value in mecab_field.vocab.itos_dict.items() \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('위', '小', '녀')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_field.vocab.itos_dict[78], chr_field.vocab.itos_dict[160], chr_field.vocab.itos_dict[188]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset, dataloader\n",
    "## 헷갈리는 부분\n",
    "bi-directional LSTM을 쓸건데 이게 다음 단어 예측하는 LM만 데이터를 구성하면 되나? 아니면 뒤에서부터 앞의 단어를 예측하는 LM도 구성해서 concat해야 하나? -> 일단 전자라고 생각하고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ELMoDataset(Dataset):\n",
    "    def __init__(self, src, mecab_field, chr_field):\n",
    "        self.src = src\n",
    "        self.mecab_field = mecab_field\n",
    "        self.chr_field = chr_field\n",
    "        self.named_tuple = namedtuple('data', ['src', 'trg', 'src_chr'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.named_tuple(self.getitem(idx), self.getitem(idx)[1:], self.getitem(idx, is_char=True))\n",
    "    \n",
    "    def getitem(self, idx, is_char=False):\n",
    "        data = self.src[idx]\n",
    "        tokenize_data = self.mecab_field.preprocess(data)\n",
    "        if is_char:\n",
    "            chrs = chr_field.preprocess(tokenize_data)\n",
    "            pad_chrs = self.chr_field.pad_process(tokenize_data)\n",
    "            return pad_chrs\n",
    "        return torch.Tensor(self.mecab_field.vocab.stoi(tokenize_data)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ELMoDataset(train, mecab_field, chr_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(_) for _ in mecab_field.vocab.stoi_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = [torch.tensor([[72,  0,  0,  0,  0]]), torch.tensor([[0, 0, 0, 0, 0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[72,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr_field.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '하', '세요', '반갑', '습니', 'ek', 'edd']\n",
      "[['안', '녕'], ['하'], ['세', '요'], ['반', '갑'], ['습', '니'], [], []]\n",
      "tensor([[ 72.,   0.,   0.,   0.,   0.],\n",
      "        [ 18.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.],\n",
      "        [176.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 210.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.]])\n"
     ]
    }
   ],
   "source": [
    "data = '안녕하세요 반갑습니ek edd'\n",
    "token_data = mecab_field.preprocess(data)\n",
    "print(token_data)\n",
    "token_chr_data = chr_field.preprocess(token_data)\n",
    "print(token_chr_data)\n",
    "process_chr = chr_field.pad_process(token_chr_data)\n",
    "print(process_chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_chr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data(src=tensor([110, 111, 112,  23, 113,  43,  44,  12, 114,   9,  11, 115,  18,  12,\n",
       "          7, 116,  19,  20, 117,  38, 118,  24,   4, 119, 120,  18,  12,   4,\n",
       "         25,   5, 121,  14, 122,  10,  13, 123, 124, 125,  23,  10,  22,  45,\n",
       "         10,  16,   5,   4, 126,  10,  25,  30,  23,   8,   9,  36,  16,  46,\n",
       "        127,  31,   6, 128,   8, 129,   9,  13,  16,  41,  45,  21,  16,   5,\n",
       "          4,  18,  12,  11, 130,  29, 131,  39,   6, 132,  17, 133,  13, 134,\n",
       "          7,  19,  11, 135, 136, 137,   8, 138, 139, 140,  46, 141, 142,  47,\n",
       "         15,  25,   6, 143,  23, 144,  14,  16,  13,   9,  13, 145, 146,  22,\n",
       "         24,  32,   8, 147,  13, 148, 149, 150,  15, 151,   9,  13,  16,   5,\n",
       "          4,  24,  32,  30, 152, 153,  22, 154,  10,   5,   4, 155, 156, 157,\n",
       "        158,  33,  15,  19,  28,  18,  12, 159,  25,  10, 160,  42, 161, 162,\n",
       "         24,  32,   8, 163,  27,   5,   4,  48, 164, 165, 166,  33,  15,  18,\n",
       "         12,   7,  19,   7, 167,   7, 168,  14, 169, 170,  17, 171,  11, 172,\n",
       "         10, 173, 174, 175,   5,   4,  43,  44, 176, 177, 178,  33,  15,  18,\n",
       "         12,   7,  19,  14, 179,  17, 180,  13, 181,   8, 182,   5,   4, 183,\n",
       "        184, 185, 186, 187, 188,  12, 189,  48,  47, 190, 191, 192, 193,  15,\n",
       "        194,  28,  19,   6, 195,  14, 196, 197,  17, 198,  26, 199,  11, 200,\n",
       "         14,  16, 201,   5,   4]), trg=tensor([111, 112,  23, 113,  43,  44,  12, 114,   9,  11, 115,  18,  12,   7,\n",
       "        116,  19,  20, 117,  38, 118,  24,   4, 119, 120,  18,  12,   4,  25,\n",
       "          5, 121,  14, 122,  10,  13, 123, 124, 125,  23,  10,  22,  45,  10,\n",
       "         16,   5,   4, 126,  10,  25,  30,  23,   8,   9,  36,  16,  46, 127,\n",
       "         31,   6, 128,   8, 129,   9,  13,  16,  41,  45,  21,  16,   5,   4,\n",
       "         18,  12,  11, 130,  29, 131,  39,   6, 132,  17, 133,  13, 134,   7,\n",
       "         19,  11, 135, 136, 137,   8, 138, 139, 140,  46, 141, 142,  47,  15,\n",
       "         25,   6, 143,  23, 144,  14,  16,  13,   9,  13, 145, 146,  22,  24,\n",
       "         32,   8, 147,  13, 148, 149, 150,  15, 151,   9,  13,  16,   5,   4,\n",
       "         24,  32,  30, 152, 153,  22, 154,  10,   5,   4, 155, 156, 157, 158,\n",
       "         33,  15,  19,  28,  18,  12, 159,  25,  10, 160,  42, 161, 162,  24,\n",
       "         32,   8, 163,  27,   5,   4,  48, 164, 165, 166,  33,  15,  18,  12,\n",
       "          7,  19,   7, 167,   7, 168,  14, 169, 170,  17, 171,  11, 172,  10,\n",
       "        173, 174, 175,   5,   4,  43,  44, 176, 177, 178,  33,  15,  18,  12,\n",
       "          7,  19,  14, 179,  17, 180,  13, 181,   8, 182,   5,   4, 183, 184,\n",
       "        185, 186, 187, 188,  12, 189,  48,  47, 190, 191, 192, 193,  15, 194,\n",
       "         28,  19,   6, 195,  14, 196, 197,  17, 198,  26, 199,  11, 200,  14,\n",
       "         16, 201,   5,   4]), src_chr=tensor([[  0,   0,   0,   0,   0],\n",
       "        [160,   0,   0,   0,   0],\n",
       "        [161,   0,   0,   0,   0],\n",
       "        ...,\n",
       "        [229,   0,   0,   0,   0],\n",
       "        [  6,   0,   0,   0,   0],\n",
       "        [  9,   0,   0,   0,   0]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0,   0,   0,   0,   0],\n",
      "        [ 13,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 13,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 69,   0,   0,   0,   0],\n",
      "        [115,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 16,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 13,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 17,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 49,   0,   0,   0,   0],\n",
      "        [  6,   0,   0,   0,   0],\n",
      "        [  9,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 74,   0,   0,   0,   0],\n",
      "        [  7,   0,   0,   0,   0],\n",
      "        [122,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 13,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 24,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 49,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 18,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 25,   0,   0,   0,   0],\n",
      "        [124,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  7,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 18,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 27,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 30,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 10,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [128,   0,   0,   0,   0],\n",
      "        [ 53,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 17,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  9,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [130,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 27,   0,   0,   0,   0],\n",
      "        [ 31,   0,   0,   0,   0],\n",
      "        [ 16,   0,   0,   0,   0],\n",
      "        [ 32,   0,   0,   0,   0],\n",
      "        [ 78,   0,   0,   0,   0],\n",
      "        [ 24,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [ 16,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 13,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  7,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  9,   0,   0,   0,   0],\n",
      "        [ 28,   0,   0,   0,   0],\n",
      "        [ 87,   0,   0,   0,   0],\n",
      "        [  5,   0,   0,   0,   0],\n",
      "        [137,   0,   0,   0,   0],\n",
      "        [ 88,   0,   0,   0,   0],\n",
      "        [  5,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 33,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 90,   0,   0,   0,   0],\n",
      "        [142,   0,   0,   0,   0],\n",
      "        [ 52,   0,   0,   0,   0],\n",
      "        [143,   0,   0,   0,   0],\n",
      "        [ 34,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 18,   0,   0,   0,   0],\n",
      "        [  6,   0,   0,   0,   0],\n",
      "        [  9,   0,   0,   0,   0],\n",
      "        [ 91,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 17,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 33,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 13,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [ 18,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [154,   0,   0,   0,   0],\n",
      "        [ 27,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0],\n",
      "        [157,   0,   0,   0,   0],\n",
      "        [ 95,   0,   0,   0,   0],\n",
      "        [ 30,   0,   0,   0,   0],\n",
      "        [ 36,   0,   0,   0,   0],\n",
      "        [  9,   0,   0,   0,   0]])\n",
      "tensor([ 49,   6,  34,   6,  50,  51,  52,  53,  26,   7,  54,   6,  55,   8,\n",
      "         56,  57,  58,  27,   5,   4,  59,  60,  11,  61,  62,   6,  35,  17,\n",
      "         63,  20,  64,  27,  65,   9,  36,  37,  28,  66,  67,  11,  68,   9,\n",
      "         69,  21,  70,  29,  35,  14,  71,  72,  30,  73,   8,  74,   4,  75,\n",
      "         34,  76,  15,  21,  77,   7,  38,  78,  17,  79,  80,  20,  81,  37,\n",
      "         20,   7,  82,   6,  83,  11,  84,   4,  85,  39,  10,  86,  87,  10,\n",
      "         22,  88,  31,  89,  90,  91,  92,  93,  94,  95,  96,  97,   9,   5,\n",
      "          4,  98,  99,   8, 100,  40,  31, 101,  40, 102,   6, 103,  26, 104,\n",
      "        105,   9,  41, 106,  21, 107, 108,  42,  29, 109,   4])\n",
      "tensor([  6,  34,   6,  50,  51,  52,  53,  26,   7,  54,   6,  55,   8,  56,\n",
      "         57,  58,  27,   5,   4,  59,  60,  11,  61,  62,   6,  35,  17,  63,\n",
      "         20,  64,  27,  65,   9,  36,  37,  28,  66,  67,  11,  68,   9,  69,\n",
      "         21,  70,  29,  35,  14,  71,  72,  30,  73,   8,  74,   4,  75,  34,\n",
      "         76,  15,  21,  77,   7,  38,  78,  17,  79,  80,  20,  81,  37,  20,\n",
      "          7,  82,   6,  83,  11,  84,   4,  85,  39,  10,  86,  87,  10,  22,\n",
      "         88,  31,  89,  90,  91,  92,  93,  94,  95,  96,  97,   9,   5,   4,\n",
      "         98,  99,   8, 100,  40,  31, 101,  40, 102,   6, 103,  26, 104, 105,\n",
      "          9,  41, 106,  21, 107, 108,  42,  29, 109,   4])\n"
     ]
    }
   ],
   "source": [
    "for _ in ds:\n",
    "    print(_.src_chr)\n",
    "    print(_.src)\n",
    "    print(_.trg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (src, trg, src_chr) = zip(*batch)\n",
    "    named_tuple = namedtuple('data', ['src', 'trg', 'src_chr'])\n",
    "    src_pad = pad_sequence(src, batch_first=True, padding_value=0)\n",
    "    trg_pad = pad_sequence(trg, batch_first=True, padding_value=0)\n",
    "    src_chr_pad = pad_sequence(src_chr, batch_first=True, padding_value=0)\n",
    "    return named_tuple(src_pad, trg_pad, src_chr_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pack_pad_collate(batch):\n",
    "#     (src, trg) = zip(*batch)\n",
    "#     src_len = torch.Tensor([len(s) for s in src])\n",
    "#     trg_len = torch.Tensor([len(t) for t in trg])\n",
    "#     named_tuple = namedtuple('data', ['src', 'trg'])\n",
    "#     src_pad = pad_sequence(src, batch_first=True, padding_value=0)\n",
    "#     trg_pad = pad_sequence(trg, batch_first=True, padding_value=0)\n",
    "#     src_pack = pack_padded_sequence(src_pad, lengths=src_len, batch_first=True, enforce_sorted=False)\n",
    "#     trg_pack = pack_padded_sequence(trg_pad, lengths=trg_len, batch_first=True, enforce_sorted=False)\n",
    "#     return named_tuple(src_pack, trg_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 243])\n",
      "torch.Size([2, 242])\n",
      "torch.Size([2, 243, 5])\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size = 16, collate_fn = pad_collate)\n",
    "for _ in dl:\n",
    "    print(_.src.data.shape)\n",
    "    print(_.trg.data.shape)\n",
    "    print(_.src_chr.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim)) \n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # torch.Size([2, 243, 5, 1024])\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "#         print(pooled_0.shape)\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1), dim = -1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(chr_field.vocab)\n",
    "EMBEDDING_DIM = 1024\n",
    "N_FILTERS = 1\n",
    "FILTER_SIZES = (1, 2)\n",
    "PAD_IDX = chr_field.vocab.stoi_dict['<PAD>']\n",
    "SPECIAL_TOKENS = chr_field.vocab.special_tokens\n",
    "SPECIAL_TOKENS_INDEX = chr_field.vocab.special_tokens_idx\n",
    "CHR_DICT = chr_dict\n",
    "OUTPUT_DIM = 1024\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hid_dim, n_layers, dropout, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.num_dircetions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout, bidirectional = bidirectional)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim * self.num_dircetions, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return torch.zeros(self.n_layers * self.num_dircetions, 1, self.hid_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "#         print(f'input shape : {input.shape}') # seqlen, batch, hid_dim(output_dim of cnn)\n",
    "        output, (hidden, cell) = self.lstm(input)\n",
    "#         print(f'output shape : {output.shape}') # ouput shape :(seq_len, batch, num_directions * hidden_size)  \n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "#         print(f'prediction shape {prediction.shape}') # seq len, batchsize, trg_dim\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = OUTPUT_DIM\n",
    "PREDICT_DIM = len(mecab_field.vocab)\n",
    "HID_DIM = 1024\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "TRG_PAD_IDX = mecab_field.vocab.stoi_dict['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LSTM_LM(INPUT_DIM, PREDICT_DIM, HID_DIM, N_LAYERS, DROPOUT, BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<sos>`토큰이랑 `<eos>` 토큰은 어떻게 CNN처리 해야하지?  -> 일단 빼는걸로 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글이랑 영어랑 다른점 : 영어는 3char이하인 단어가 별로 없는데 한글은 1~2개로 많이 끊겨서 conv연산 하기가 애매함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.apply(initialize_weights);\n",
    "rnn.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 242 is out of bounds for dimension 1 with size 242",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-b508bf38b0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 242 is out of bounds for dimension 1 with size 242"
     ]
    }
   ],
   "source": [
    "trg[:, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2792, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3381, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0013, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3328, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3390, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2971, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2536, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4594, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2593, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2954, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0752, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3759, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1159, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3165, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3210, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1894, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3504, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3150, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2777, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2833, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2728, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0285, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2741, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2412, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2808, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0833, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1306, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3432, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1917, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3783, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2802, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3282, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9100, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2726, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4429, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3250, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3652, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3628, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2070, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4055, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9704, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3040, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3013, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1771, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1739, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1942, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2816, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3682, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5385, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2074, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1292, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9733, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3144, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2439, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2941, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2993, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2866, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1355, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3172, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3113, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2627, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9191, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4239, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1748, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3278, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3195, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2698, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3001, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2827, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0952, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2458, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2252, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2544, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3184, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2939, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3529, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1209, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3165, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3896, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0822, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2947, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2730, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1697, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2990, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4416, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2644, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5756, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3238, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4167, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0330, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3910, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3013, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2792, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3117, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2735, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0042, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2819, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2807, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2821, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1782, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2404, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3094, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2084, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2536, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2240, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0200, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2699, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0916, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3015, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3010, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2450, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4000, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0269, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1539, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3306, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2669, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2947, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3680, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3494, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3381, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3796, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9207, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3145, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8960, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0267, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7921, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8889, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2768, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2768, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9527, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0805, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9285, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1054, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8867, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6347, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8538, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3034, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2967, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3345, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7661, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9055, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9402, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8182, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8686, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0256, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0035, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9405, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3057, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7242, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2485, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0243, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8748, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2670, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7079, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3055, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4525, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8423, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2855, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5625, grad_fn=<NllLossBackward>)\n",
      "tensor(4.3917, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2919, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1219, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8053, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1833, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9167, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6795, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0208, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9894, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7507, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6469, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7601, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7510, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0636, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1225, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2728, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6183, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3089, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8661, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7317, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6952, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2622, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9305, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2823, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7705, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7155, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6227, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4246, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0589, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7482, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2700, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0818, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6937, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4998, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2880, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7249, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7115, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0681, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7107, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8147, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7904, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0352, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8814, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6533, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7206, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8808, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7338, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4389, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6075, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7187, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8487, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8093, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6268, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1661, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5505, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6951, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3049, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6837, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6383, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1207, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0870, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1381, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2813, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2455, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7062, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6765, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6891, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8781, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6615, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2036, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2795, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2311, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5252, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2143, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0658, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5071, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2426, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9731, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6754, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6416, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7124, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6106, grad_fn=<NllLossBackward>)\n",
      "tensor(4.2964, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0979, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3349, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9044, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4471, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3361, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3542, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1104, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3487, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0916, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2780, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2742, grad_fn=<NllLossBackward>)\n",
      "tensor(5.5108, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9579, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3793, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2527, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3002, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1439, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2104, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2558, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3815, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3330, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8795, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3535, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3021, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1542, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0132, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0391, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3296, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1528, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3457, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3382, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4120, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8095, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0696, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4759, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2918, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3280, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2654, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2861, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3388, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1460, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2871, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3159, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2766, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0972, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3181, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3255, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3630, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3700, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2194, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2642, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0103, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2987, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0802, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2187, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2590, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2369, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2669, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2286, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3107, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2026, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7749, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3341, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2357, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4357, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3363, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3174, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2625, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2587, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2873, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2562, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3204, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0651, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3266, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2038, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3706, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0385, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2467, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3214, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3588, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2747, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1128, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2703, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3065, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3144, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2318, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4332, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3902, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2969, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0768, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3317, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3735, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2968, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2927, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4077, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0266, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3367, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2448, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2512, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2101, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0869, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0782, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3265, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2708, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2109, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2100, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2495, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0450, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4609, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2789, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2883, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4413, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2444, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0909, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2793, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1166, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3023, grad_fn=<NllLossBackward>)\n",
      "tensor(5.4307, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2544, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2990, grad_fn=<NllLossBackward>)\n",
      "tensor(5.3490, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0007, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1840, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2368, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2004, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7901, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9166, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9938, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2669, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9417, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7422, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6871, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7946, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7784, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4282, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2004, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7267, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9674, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7255, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2859, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2292, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6562, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2555, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2387, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2317, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9440, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8955, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8270, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2282, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0440, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0173, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8762, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6936, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2111, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9614, grad_fn=<NllLossBackward>)\n",
      "tensor(4.4196, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2468, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0989, grad_fn=<NllLossBackward>)\n",
      "tensor(4.5738, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1957, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7407, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9238, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0314, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7413, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1004, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0233, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6178, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1902, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9319, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7295, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6813, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2633, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9632, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0970, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2182, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0393, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2659, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6800, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2250, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7201, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9855, grad_fn=<NllLossBackward>)\n",
      "tensor(4.9111, grad_fn=<NllLossBackward>)\n",
      "tensor(5.0573, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2519, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8938, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1630, grad_fn=<NllLossBackward>)\n",
      "tensor(4.6951, grad_fn=<NllLossBackward>)\n",
      "tensor(5.2024, grad_fn=<NllLossBackward>)\n",
      "tensor(4.7171, grad_fn=<NllLossBackward>)\n",
      "tensor(4.8214, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-46de1c3db43d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msrc_chr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_chr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 토큰 별 캐릭터가 먼저 나오도록\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_c\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_chr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "n_pass = 0\n",
    "for epoch in range(5):\n",
    "    optimizer.zero_grad()\n",
    "    for src, trg, src_chr in dl:\n",
    "        src_chr = src_chr.permute(1, 0, 2) # 토큰 별 캐릭터가 먼저 나오도록\n",
    "        rnn.init_hidden()\n",
    "        for idx, src_c in enumerate(src_chr):\n",
    "            features = cnn(src_c.unsqueeze(1))\n",
    "            output, hidden, cell = rnn(features.unsqueeze(1))\n",
    "#             print(idx, len(trg))\n",
    "            try:\n",
    "                loss = criterion(output.squeeze(1), trg[:, idx])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += [loss.item()]\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해야 할 일 : 1) for 문 안돌고 배치 처리할 수 있게 하기 2) CNN + RNN 감싸기\n",
    "1) 배치별로 토큰 내에 있는 캐릭터 글자에 따라 CNN길이가 다른데 어떻게 처리하지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long36v",
   "language": "python",
   "name": "long36v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
