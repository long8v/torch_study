## ğŸ¤— Result
### Environment
```
mlflow == 1.15.0
torch == 1.7.1
torchtext == 0.8.1
pytorch-lightning == 1.2.8
tokenizers == 0.9.3
pytorch-crf == 0.7.2
Korpora == 0.2
```

### pretraining
**1) run**

1-1) data loading

Korporaì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ìš´ ë°›ì•„ ë¬¸ì„œ ë‹¨ìœ„ì˜ listë¡œ í”¼í´ë§í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤
ì½”ë“œ ë‚´ì— ì†ŒìŠ¤ íŒŒì¼ ì €ì¥í•˜ëŠ” ê²½ë¡œë¥¼ ë°”ê¿”ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤ 
```
python source/load_korpora.py
>>> yes (í˜¹ì€ ì›í•˜ëŠ” ë¬¸ì„œ ê°œìˆ˜ë§Œí¼ì˜ ìˆ«ì)
```

1-2) bert í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ / tokenizer í•™ìŠµ

ë¬¸ì„œ ë‹¨ìœ„ë¡œ train / validë¥¼ ë‚˜ëˆ„ê³  NSP predictionì„ í•  ë•Œ ë¬¸ì„œ ë§¨ ë ë¬¸ì¥ê³¼ ë‹¤ìŒ ë¬¸ì„œ ë§¨ ì²« ë¬¸ì¥ì´ NSP=1ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë©´ ì•ˆë˜ë¯€ë¡œ, ë¬¸ì¥ ëì— [EOD]ë¼ëŠ” í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤. <br>
í† í¬ë‚˜ì´ì €ì˜ ê²½ìš° wordpieceë¡œë§Œ í•™ìŠµí•˜ê²Œë˜ë©´ í•œêµ­ì–´ì˜ ê²½ìš° ì¡°ì‚¬ ë“± ë‹¤ë¹ˆë„ í† í°ì´ ë¶™ëŠ” ê²½ìš°ê°€ ë§ì•„ mecabìœ¼ë¡œ í˜•íƒœì†Œë¡œ ìë¥´ê³  ì˜ë¦° ë¶€ë¶„ì— ##ì„ ë¶™ì¸ ë‹¤ìŒ tokenizersì˜ `wordpieces_prefix='##'`  argumentë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ì´ë¥¼ ë§‰ì„ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
```
python source/tokenizer.py
```
ì €ì¥ ê²½ë¡œì— tokenizer ëª¨ë¸ì´ ì €ì¥ë˜ê³  ì´ íŒŒì¼ì´ vocab ìˆ˜, ë¬¸ì¥->ì¸ë±ìŠ¤ ë”•ì…”ë„ˆë¦¬, ì¸ë±ìŠ¤->ë¬¸ì¥ ë”•ì…”ë„ˆë¦¬, ìŠ¤í˜ì…œ í† í°ë“±ì„ ë‹´ê³  ìˆê¸° ë•Œë¬¸ì— ì´í›„ ê°„í¸í•˜ê²Œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1-3) config.yaml ìˆ˜ì •
```
data:
    src: '/home/long8v/torch_study/paper/file/bert/bert.txt' # í•™ìŠµë°ì´í„°
    src_valid: '/home/long8v/torch_study/paper/file/bert/bert_valid.txt' # validataion ë°ì´í„°
    vocab:  '/home/long8v/torch_study/paper/file/bert/vocab.json' # vocab ê°ì²´
    max_len: 128 # ìµœëŒ€ í† í° ê°œìˆ˜
    nsp_prob: 0.5   # nsp = 1ì¼ í™•ë¥ (0.5ì´ë©´ nsp=1, 0 ë¹„ìœ¨ì´ 1:1ì´ë¼ëŠ” ê²ƒ) 
    mask_ratio: 0.1 # MLMì„ ìœ„í•œ ë§ˆìŠ¤í‚¹ í† í° ë¹„ìœ¨
    batch_size: 64 
model:
    hid_dim: 256
    n_layers: 2
    n_heads: 8
    pf_dim: 512
    dropout: 0.5

train:
    n_epochs: 1000
    device: 'cuda'
    lr: 0.0005
    scheduler: True
    warmup_steps: 100
    train_mlm: True   # mlm í•™ìŠµ ì—¬ë¶€ 
    train_nsp: False  # nsp í•™ìŠµ ì—¬ë¶€
```

1-4) ì‹¤í–‰
```
python run.py
```
ì‹¤í–‰ì„ í•˜ê²Œ ë˜ë©´ mlflow + pytorch-lightning í”„ë ˆì„ì›Œí¬ì—ì„œ í•™ìŠµì´ ë©ë‹ˆë‹¤. 
ìœ„ íŒŒì¼ ì‹¤í–‰ ê²½ë¡œì—ì„œ ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ í•™ìŠµ ê²°ê³¼ë“¤ì„ ì›¹ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```
mlflow ui
```

ëª¨ë¸ í•™ìŠµ metric, checkpointëŠ” `mlruns/0`í´ë” ë‚´ì—ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [ì˜ˆì‹œ](https://github.com/long8v/torch_study/tree/master/paper/06_BERT/bert_example/4035dd4c47fe43c6a507c0d74365211b)


**2) data**
Korporaì—ì„œ ì œê³µí•˜ëŠ” í•œêµ­ì–´ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
[petetion data](https://github.com/lovit/petitions_archive), [namu-wiki data](https://github.com/lovit/namuwikitext)

**3) model**

```
  | Name          | Type             | Params
---------------------------------------------------
0 | encoder       | Encoder          | 4.1 M 
1 | nsp           | Linear           | 514   
2 | mlm           | Linear           | 3.0 M 
3 | criterion_nsp | CrossEntropyLoss | 0     
4 | criterion_mlm | CrossEntropyLoss | 0     
---------------------------------------------------
7.0 M     Trainable params
0         Non-trainable params
7.0 M     Total params
28.101    Total estimated model params size (MB)
```
transformer êµ¬ì¡° ìì²´ëŠ” 4ì›” ë ˆí¼ëŸ°ìŠ¤ ì½”ë“œì˜€ë˜ [transformer](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb)

**4) result**

|metric|train|valid|
|:---:|:---:|:---:|
|MLM loss|2.892|4.854|
|NSP loss|0.221|1.067|
|MLM accuracy|0.46|0.40|
|NSP accuracy|0.88|0.55|

**5) experiment**

5-1) scheduler
![image](https://user-images.githubusercontent.com/46675408/127120275-6b6dc8d4-1d85-4a80-afe6-28bf65db5906.png)

### finetuning
**1) task**

BERTì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ finetune-taskë¡œ NERë¥¼ ì„ íƒí•˜ì˜€ìŠµë‹ˆë‹¤

**2) run **
2-1) 
```
python run_finetune.py
```

**3) data**

KLUE ë²¤ì¹˜ë§ˆí¬ì˜ NER ë°ì´í„°ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤

[KLUE NER](https://github.com/KLUE-benchmark/KLUE/tree/main/klue_benchmark/klue-ner-v1)

**4) model size**

```
  | Name    | Type    | Params
------------------------------------
0 | encoder | Encoder | 4.1 M 
1 | fcn     | Linear  | 3.6 K 
2 | crf     | CRF     | 224   
------------------------------------
4.1 M     Trainable params
0         Non-trainable params
4.1 M     Total params
16.222    Total estimated model params size (MB)
```

**5) result**

|metric|train|valid|
|:---:|:---:|:---:|
|loss|13.49|61.15|
|micro F1|0.991|0.922|
|macro F1|0.931|0.791|

**6) experiment**

![image](https://user-images.githubusercontent.com/46675408/127128705-32bca8f5-f099-492d-85be-d5bd7b51d3e6.png)


## ğŸ¤” Paper review
**1) PPT í•œ ì¥ ë¶„ëŸ‰ìœ¼ë¡œ ììœ ë¡­ê²Œ ë…¼ë¬¸ ì •ë¦¬ ë’¤ ì´ë¯¸ì§€ë¡œ ì²¨ë¶€**



**2) (ìŠ¬ë™ìœ¼ë¡œ ì´ë¯¸ í† ë¡ ì„ í–ˆì§€ë§Œ ê·¸ë˜ë„) ì´í•´ê°€ ì•ˆ ê°€ëŠ” ë¶€ë¶„, ì´í•´ê°€ ì•ˆ ê°€ëŠ” ì´ìœ (ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**<BR>
. ì§€ë‚œ ë‹¬ í† ë¡  ì£¼ì œ : BERTëŠ” ë¬¸ë§¥ì— ë”°ë¼ ê°™ì€ **í† í°ì˜ í‘œí˜„ ê°’**ì´ ë‹¬ë¼ì§€ëŠ”ê°€?<br>
token embedding â†’ ê°™ê² ì£ <br>
positonal embedding â†’ ë‹¬ë¼ì§€ê² ì£ <br>
segment embedding â†’ ë‹¬ë¼ì§€ê² ì£ <br>
======================<br>
attention í•˜ê³  ë‚œ ë’¤ì—ëŠ” â†’ ë‹¬ë¼ì§€ê² ì£ 

ë‹¬ë¼ì§„ë‹¤.
  
. MLMì„ ì •í™•íˆ ì–´ë–»ê²Œ í•˜ëŠ”ê±´ì§€ ëª¨ë¥´ê² ìŒ mask í† í° ë„£ì€ì±„ë¡œ ì¸í’‹ì— ë„£ì€ ë’¤ì— mask í† í°ì— ëŒ€í•´ì„œë§Œ predict í•˜ê²Œ í•˜ë ¤ë©´ ì–´ë–»ê²Œ êµ¬í˜„í•´ì•¼í•˜ì§€?<br>
. [MASK] í† í°ì„ 15% ì„ ì •í•˜ê³  80%ì€ ë°”ê¾¸ê³  10%ì€ ì¹˜í™˜í•˜ê³  **10%ëŠ” ê·¸ëƒ¥ ë‘”ê±°..** 10% ê·¸ëƒ¥ ë‘ëŠ”ê²Œ ì˜ë¯¸ê°€ ë­ì§€ ê± ë‘”ê±´ê°€..<BR>

original :    i go to shcool.<BR>
inference :    i [mask] to **[mask].** 15%<BR>
input :        **i [mask] to school.**<BR>
. ì„ ì •ëœ [MASK]ë“¤ì˜ predictionì„ í•  ë•Œ ê·¸ëƒ¥ ë‘” í† í°ì˜ ì„ë² ë”©ì´ ë“¤ì–´ê°

**3) ì¬ë°Œì—ˆë˜ ë¶€ë¶„**<BR>
. MLM ì•„ì´ë””ì–´ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì„. <BR>
. BERTbaseê°€ GPTë‘ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ë§ì¶°ì„œ ë‚˜ì˜¨ ê²ƒì¸ ê²ƒ..ã…ã…<BR>
. ELMo ì–˜ê¸° ë‚˜ì˜¤ëŠ”ë° ì™ ì§€ ê³ í–¥ì¹œêµ¬ ë§Œë‚œë“¯í•œ ë°˜ê°€ì›€..<BR>
. **feature base**ë‘ **fine-tuning**ì´ë‘ ê¹”ë”í•˜ê²Œ ë‚˜ëˆ ì¤€ê±°. í•­ìƒ fine-tuning ì–˜ê¸°í•˜ë©´ ë…¼ë€ì´ ë˜ëŠ” ë¶€ë¶„ì´ì—ˆìŒ.<BR>
. ê·¸ë¦¬ê³  ë˜ feature baseë¡œ ë¬¸ì œë¥¼ í’€ì–´ì£¼ì‹¬<BR>
. ë°ì´í„°ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ SHUFFLEí•œ ë°ì´í„°ì…‹ë³´ë‹¤ëŠ” DOCUMENT ë‹¨ìœ„ì˜ ë°ì´í„°ì…‹ì„ ì¨ì•¼ì§€ ê¸´ ë¬¸ë§¥ì„ ë½‘ì•„ë‚¼ìˆ˜ ìˆì–´ì„œ ì¢‹ë‹¤ê³  í•¨.<BR>
  
**4) ë…¼ë¬¸ êµ¬í˜„ ì‹œ ì£¼ì˜í•´ì•¼í•  ê²ƒ ê°™ì€ ë¶€ë¶„(ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**<BR>
- input ì„ ë§Œë“œëŠ” ê²ƒ(segment token, mask, CLS, SEP ë“±.. **ìµœì†Œ 1ì£¼ ê±¸ë¦¼..**)
- MLM 
- transformer êµ¬ì¡°
- ëª¨ë¸ì´ ë³µì¡í•œê±´ ì•„ë‹Œë° ì´ê²ƒì €ê²ƒ ë””í…Œì¼ì´ ë§ì•„ì„œ ì²˜ìŒì— êµ¬ì¡°ë¥¼ ì˜ ì§œë†“ìœ¼ë©´ í¸í• ê²ƒ ê°™ë‹¤

## ğŸ¤« ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê²Œ êµ¬í˜„í•œ ë¶€ë¶„
  
- í•œêµ­ì–´ ë°ì´í„°
- optimizer : AdamW
- scheduler : 
- ë¬¸ì¥ì´ ê¸¸ ë•Œ max_seq_lenì„ ìë¥´ëŠ” ë¶€ë¶„ ? senBë¥¼ ë¨¼ì € ìë¥´ë„ë¡ í–ˆëŠ”ë° ë…¼ë¬¸ì—ì„  ì–´ë–»ê²Œ ìë¥´ëŠ”ì§€ ë‚˜ì™€ìˆì§„ ì•ŠìŒ
  
## ğŸ¤­ ë…¼ë¬¸ êµ¬í˜„í•˜ë©´ì„œ ë°°ìš´ ì  / ëŠë‚€ ì 
 
- BERT, transformerì˜ seq_len ì°¨ì›ì€ ì •í•´ì ¸ìˆì„ í•„ìš”ê°€ ì—†ê³  max_lenìœ¼ë¡œ ë°°ì¹˜ë³„ë¡œ íŒ¨ë”©í•˜ëŠ”ê±°ë‹¤<br>
  -> ì–´ì°¨í”¼ ë§ˆì§€ë§‰ ì°¨ì›ì€ hid_dimì´ê³  ê·¸ê±¸ë¡œ ì—°ì‚°ì„ í•¨<br>
  -> seq_len ì°¨ì›ì€ ë‚¨ì•„ìˆìŒ, transformerì˜ encoderì˜ ouput ì°¨ì›ì€ [batch size, src len, hid dim]<br>
  -> max_seq_lenì€ OOMì„ ë§‰ê¸° ìœ„í•´ ìˆëŠ”ê±°ë‹¤<br>
- NSPëŠ” ê°™ì€ ë¬¸ë‹¨ì—ì„œë„ ë°”ë¡œ ë‹¤ìŒ ë¬¸ì¥ì´ ì•„ë‹ˆë©´ 0ì´ë‹¤ 
- [mask] ì¤‘ì— 10%ëŠ” ê·¸ëŒ€ë¡œ ë‚¨ê¸°ëŠ” ì´ìœ  : [mask]ë¡œ ì„ íƒëœ í† í°ì€ ì˜ˆì¸¡ì„ í•˜ëŠ”ë°, inputì´ [mask]ì´ê±°ë‚˜, randomì´ê±°ë‚˜ ê·¸ëŒ€ë¡œ í† í°ì¼ ë•Œ ë§ì¶”ëŠ” ê²ƒ
- bertì—ì„œ senA, senBì˜ ë‹¨ìœ„ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì„. finetune taskì—ì„œ QAë“±ì„ í• ë•Œì—ëŠ” ë¬¸ë‹¨ì´ë‚˜ ë‹¨ì–´ê°€ ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŒ.
- ë°ì´í„°ê°€ í´ ë•Œ ë¦¬ìŠ¤íŠ¸ì— ì˜¬ë¦¬ê±°ë‚˜ í•˜ë©´ ë©”ëª¨ë¦¬ ì—ëŸ¬ê°€ ë‚¨..í° ë°ì´í„°ì…‹ì„ ë‹¤ë£° ë•ŒëŠ” ê¸°ì¡´ì˜ ë°©ë²•ì´ë‘ ë‹¤ë¥´ê²Œ í•´ì•¼í•¨ -> linecache ì‚¬ìš©í•´ë´„
- finetuningì„ í•  ë•Œ, token embeddingë§Œ ì‚¬ìš©í•œë‹¤ê³  ìƒê°í–ˆì—ˆëŠ”ë° ê·¸ê²Œ ì•„ë‹Œ attention ê°’(batch_size, seq_len, hidden_dim)ì„ ì‚¬ìš©í•¨!
- AdamWì˜ ìœ„ë ¥..initializeì˜ ìœ„ë ¥..
- tokenizers ì‚¬ìš©ë²•
- tokenizers ì‚¬ìš©í•˜ì—¬ chr-level NER input/output ë§Œë“œëŠ” ì½”ë“œ!
