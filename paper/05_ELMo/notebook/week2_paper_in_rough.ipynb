{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korpora.fetch('namuwikitext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = Korpora.load('namuwikitext')\n",
    "# with open('kor.p', 'wb') as f:\n",
    "#     pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kor.p', 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mecab\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "import sys\n",
    "sys.path.append('../source')\n",
    "from txt_cleaner.clean.master import MasterCleaner\n",
    "from txt_cleaner.utils import *\n",
    "from torch8text.data import Vocab, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## field 1: mecab 사용 field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = mecab.MeCab()\n",
    "\n",
    "def tokenize_pos(inp):\n",
    "    if type(inp) == str:\n",
    "        return pos.morphs(inp)\n",
    "    if type(inp) == list:\n",
    "        return [tokenize_pos(i) for i in inp]\n",
    "# pos.morphs(['안녕하세요'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['안녕', '하', '세요'], [['안녕', '하', '세요'], ['안녕', '?']])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_pos('안녕하세요'), tokenize_pos(['안녕하세요', '안녕?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaner\n",
    "https://github.com/tndls9304/nlp_torch_study/tree/master/txt_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 1 dictionary is read from ../source/txt_cleaner/cleaner_config.json\n"
     ]
    }
   ],
   "source": [
    "config = json_reader('../source/txt_cleaner/cleaner_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'minimum_space_count': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['minimum_space_count'] = 2\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕하세요? 반갑습니다! 행복하세요'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner = MasterCleaner(config)\n",
    "cleaner.cleaning('안녕하세요? 반갑습니다! 행복하세요~**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['안녕', '하', '세요'], ['안녕']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_pos(['안녕하세요', '안녕'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_field = Field(tokenize = tokenize_pos, \n",
    "                 preprocessing = cleaner.cleaning,\n",
    "                    init_token = False,\n",
    "                    eos_token = False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '하', '세요', '룰루랄라']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = [text for text in corpus.train.texts if cleaner.cleaning(text)]\n",
    "mecab_field.build_vocab(train)\n",
    "mecab_field.preprocess('안녕하세요 룰루랄라 ㅇㅇㄹ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## field 2:  chr-level field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner.cleaning('아')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = MasterCleaner({'minimum_space_count':0})\n",
    "chr_field = Field(tokenize = list, \n",
    "                 preprocessing = lambda e: cleaner.cleaning(e) if len(e) > 1 else e,\n",
    "                  init_token = False,\n",
    "                  eos_token = False,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr_field.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr_field.process('안녕하세요')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset, dataloader\n",
    "## 헷갈리는 부분\n",
    "bi-directional LSTM을 쓸건데 이게 다음 단어 예측하는 LM만 데이터를 구성하면 되나? 아니면 뒤에서부터 앞의 단어를 예측하는 LM도 구성해서 concat해야 하나? -> 일단 전자라고 생각하고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ELMoDataset(Dataset):\n",
    "    def __init__(self, src, mecab_field, chr_field):\n",
    "        self.src = src\n",
    "        self.mecab_field = mecab_field\n",
    "        self.chr_field = chr_field\n",
    "        self.named_tuple = namedtuple('data', ['src', 'trg', 'src_chr'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.named_tuple(self.getitem(idx), self.getitem(idx)[1:], self.getitem(idx, is_char=True))\n",
    "    \n",
    "    def getitem(self, idx, is_char=False):\n",
    "        data = self.src[idx]\n",
    "        tokenize_data = self.mecab_field.preprocess(data)\n",
    "        if is_char:\n",
    "            chrs = chr_field.preprocess(tokenize_data)\n",
    "            pad_chrs = self.chr_field.pad_process(tokenize_data, max_len = 3)\n",
    "            return pad_chrs\n",
    "        return torch.Tensor(self.mecab_field.vocab.stoi(tokenize_data)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ELMoDataset(train, mecab_field, chr_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(_) for _ in mecab_field.vocab.stoi_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X  = [torch.tensor([[72,  0,  0,  0,  0]]), torch.tensor([[0, 0, 0, 0, 0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[72,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '하', '세요', '반갑', '습니', 'ek', 'edd']\n",
      "[['안', '녕'], ['하'], ['세', '요'], ['반', '갑'], ['습', '니'], [], []]\n",
      "tensor([[ 72,   1,   0],\n",
      "        [ 18,   0,   0],\n",
      "        [  1,   1,   0],\n",
      "        [176,   1,   0],\n",
      "        [  1, 210,   0]])\n"
     ]
    }
   ],
   "source": [
    "data = '안녕하세요 반갑습니ek edd'\n",
    "token_data = mecab_field.preprocess(data)\n",
    "print(token_data)\n",
    "token_chr_data = chr_field.preprocess(token_data)\n",
    "print(token_chr_data)\n",
    "process_chr = chr_field.pad_process(token_chr_data, max_len = 3)\n",
    "print(process_chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_chr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   0,   0],\n",
      "        [ 13,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 13,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 69,   0,   0],\n",
      "        [115,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 16,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 13,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 17,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 49,   0,   0],\n",
      "        [  6,   0,   0],\n",
      "        [  9,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 74,   0,   0],\n",
      "        [  7,   0,   0],\n",
      "        [122,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 13,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 24,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 14,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 49,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 18,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 25,   0,   0],\n",
      "        [124,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  7,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 18,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 27,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 30,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 10,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [128,   0,   0],\n",
      "        [ 53,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 17,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  9,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [130,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 27,   0,   0],\n",
      "        [ 31,   0,   0],\n",
      "        [ 16,   0,   0],\n",
      "        [ 32,   0,   0],\n",
      "        [ 78,   0,   0],\n",
      "        [ 24,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 14,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 14,   0,   0],\n",
      "        [ 16,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 13,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  7,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  9,   0,   0],\n",
      "        [ 28,   0,   0],\n",
      "        [ 87,   0,   0],\n",
      "        [  5,   0,   0],\n",
      "        [137,   0,   0],\n",
      "        [ 88,   0,   0],\n",
      "        [  5,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 33,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 90,   0,   0],\n",
      "        [142,   0,   0],\n",
      "        [ 52,   0,   0],\n",
      "        [143,   0,   0],\n",
      "        [ 34,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 18,   0,   0],\n",
      "        [  6,   0,   0],\n",
      "        [  9,   0,   0],\n",
      "        [ 91,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 17,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 33,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 13,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [ 18,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [154,   0,   0],\n",
      "        [ 27,   0,   0],\n",
      "        [  1,   0,   0],\n",
      "        [157,   0,   0],\n",
      "        [ 95,   0,   0],\n",
      "        [ 30,   0,   0],\n",
      "        [ 36,   0,   0],\n",
      "        [  9,   0,   0]])\n",
      "tensor([ 49,   6,  34,   6,  50,  51,  52,  53,  26,   7,  54,   6,  55,   8,\n",
      "         56,  57,  58,  27,   5,   4,  59,  60,  11,  61,  62,   6,  35,  17,\n",
      "         63,  20,  64,  27,  65,   9,  36,  37,  28,  66,  67,  11,  68,   9,\n",
      "         69,  21,  70,  29,  35,  14,  71,  72,  30,  73,   8,  74,   4,  75,\n",
      "         34,  76,  15,  21,  77,   7,  38,  78,  17,  79,  80,  20,  81,  37,\n",
      "         20,   7,  82,   6,  83,  11,  84,   4,  85,  39,  10,  86,  87,  10,\n",
      "         22,  88,  31,  89,  90,  91,  92,  93,  94,  95,  96,  97,   9,   5,\n",
      "          4,  98,  99,   8, 100,  40,  31, 101,  40, 102,   6, 103,  26, 104,\n",
      "        105,   9,  41, 106,  21, 107, 108,  42,  29, 109,   4])\n",
      "tensor([  6,  34,   6,  50,  51,  52,  53,  26,   7,  54,   6,  55,   8,  56,\n",
      "         57,  58,  27,   5,   4,  59,  60,  11,  61,  62,   6,  35,  17,  63,\n",
      "         20,  64,  27,  65,   9,  36,  37,  28,  66,  67,  11,  68,   9,  69,\n",
      "         21,  70,  29,  35,  14,  71,  72,  30,  73,   8,  74,   4,  75,  34,\n",
      "         76,  15,  21,  77,   7,  38,  78,  17,  79,  80,  20,  81,  37,  20,\n",
      "          7,  82,   6,  83,  11,  84,   4,  85,  39,  10,  86,  87,  10,  22,\n",
      "         88,  31,  89,  90,  91,  92,  93,  94,  95,  96,  97,   9,   5,   4,\n",
      "         98,  99,   8, 100,  40,  31, 101,  40, 102,   6, 103,  26, 104, 105,\n",
      "          9,  41, 106,  21, 107, 108,  42,  29, 109,   4])\n"
     ]
    }
   ],
   "source": [
    "for _ in ds:\n",
    "    print(_.src_chr)\n",
    "    print(_.src)\n",
    "    print(_.trg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    (src, trg, src_chr) = zip(*batch)\n",
    "    named_tuple = namedtuple('data', ['src', 'trg', 'src_chr'])\n",
    "    src_pad = pad_sequence(src, batch_first=True, padding_value=0)\n",
    "    trg_pad = pad_sequence(trg, batch_first=True, padding_value=0)\n",
    "    src_chr_pad = pad_sequence(src_chr, batch_first=True, padding_value=0)\n",
    "    return named_tuple(src_pad, trg_pad, src_chr_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pack_pad_collate(batch):\n",
    "#     (src, trg) = zip(*batch)\n",
    "#     src_len = torch.Tensor([len(s) for s in src])\n",
    "#     trg_len = torch.Tensor([len(t) for t in trg])\n",
    "#     named_tuple = namedtuple('data', ['src', 'trg'])\n",
    "#     src_pad = pad_sequence(src, batch_first=True, padding_value=0)\n",
    "#     trg_pad = pad_sequence(trg, batch_first=True, padding_value=0)\n",
    "#     src_pack = pack_padded_sequence(src_pad, lengths=src_len, batch_first=True, enforce_sorted=False)\n",
    "#     trg_pack = pack_padded_sequence(trg_pad, lengths=trg_len, batch_first=True, enforce_sorted=False)\n",
    "#     return named_tuple(src_pack, trg_pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 243])\n",
      "torch.Size([2, 242])\n",
      "torch.Size([2, 243, 3])\n"
     ]
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size = 16, collate_fn = pad_collate)\n",
    "for _ in dl:\n",
    "    print(_.src.data.shape)\n",
    "    print(_.trg.data.shape)\n",
    "    print(_.src_chr.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim)) \n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        # torch.Size([2, 243, 5, 1024])\n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "\n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "#         print(pooled_0.shape)\n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1), dim = -1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(chr_field.vocab)\n",
    "EMBEDDING_DIM = 1024\n",
    "N_FILTERS = 1\n",
    "FILTER_SIZES = (1, 2)\n",
    "PAD_IDX = chr_field.vocab.stoi_dict['<PAD>']\n",
    "SPECIAL_TOKENS = chr_field.vocab.special_tokens\n",
    "SPECIAL_TOKENS_INDEX = chr_field.vocab.special_tokens_idx\n",
    "CHR_DICT = chr_dict\n",
    "OUTPUT_DIM = 1024\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_LM(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hid_dim, n_layers, dropout, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.num_dircetions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout, bidirectional = bidirectional)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim * self.num_dircetions, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # (num_layers * num_directions, batch, hidden_size)\n",
    "        return torch.zeros(self.n_layers * self.num_dircetions, 1, self.hid_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "#         print(f'input shape : {input.shape}') # seqlen, batch, hid_dim(output_dim of cnn)\n",
    "        output, (hidden, cell) = self.lstm(input)\n",
    "#         print(f'output shape : {output.shape}') # ouput shape :(seq_len, batch, num_directions * hidden_size)  \n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "#         print(f'prediction shape {prediction.shape}') # seq len, batchsize, trg_dim\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = OUTPUT_DIM\n",
    "PREDICT_DIM = len(mecab_field.vocab)\n",
    "HID_DIM = 1024\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True\n",
    "TRG_PAD_IDX = mecab_field.vocab.stoi_dict['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = LSTM_LM(INPUT_DIM, PREDICT_DIM, HID_DIM, N_LAYERS, DROPOUT, BIDIRECTIONAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`<sos>`토큰이랑 `<eos>` 토큰은 어떻게 CNN처리 해야하지?  -> 일단 빼는걸로 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글이랑 영어랑 다른점 : 영어는 3char이하인 단어가 별로 없는데 한글은 1~2개로 많이 끊겨서 conv연산 하기가 애매함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.apply(initialize_weights);\n",
    "rnn.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = []\n",
    "n_pass = 0\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    for src, trg, src_chr in dl:\n",
    "        src_chr = src_chr.permute(1, 0, 2) # 토큰 별 캐릭터가 먼저 나오도록\n",
    "        for idx, src_c in enumerate(src_chr):\n",
    "            features = cnn(src_c.unsqueeze(1))\n",
    "            output, hidden, cell = rnn(features.unsqueeze(1))\n",
    "            try:\n",
    "                loss = criterion(output.squeeze(1), trg[:, idx])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "    epoch_loss += [loss.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해야 할 일 : 1) init hidden 2) CNN + RNN 감싸기\n",
    "1) 배치별로 토큰 내에 있는 캐릭터 글자에 따라 CNN길이가 다른데 어떻게 처리하지? -> 패딩으로 처리함..근데 이게 맞는지 모르겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "long36v",
   "language": "python",
   "name": "long36v"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
