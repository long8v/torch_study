## ğŸ¤— Result
WIP

## ğŸ¤” Paper review

**1) PPT í•œ ì¥ ë¶„ëŸ‰ìœ¼ë¡œ ììœ ë¡­ê²Œ ë…¼ë¬¸ ì •ë¦¬ ë’¤ ì´ë¯¸ì§€ë¡œ ì²¨ë¶€**
![image](https://user-images.githubusercontent.com/46675408/121776733-7c58ee00-cbc9-11eb-8c31-ede9ffc29e88.png)


**2) (ìŠ¬ë™ìœ¼ë¡œ ì´ë¯¸ í† ë¡ ì„ í–ˆì§€ë§Œ ê·¸ë˜ë„) ì´í•´ê°€ ì•ˆ ê°€ëŠ” ë¶€ë¶„, ì´í•´ê°€ ì•ˆ ê°€ëŠ” ì´ìœ (ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**

ã„±. 3.2.3 self attentionì„ ì‚¬ìš©í•˜ë©´ previous layerì˜ outputì´ K, V, Qê°€ ëœë‹¤ëŠ”ê±´ê°€

The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the **previous layer** in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder 

self-attention + FFNì´ ëª‡ì¸µìœ¼ë¡œ ìŒ“ëŠ”ê±´ë° í•œ layerì˜ outputì´ ì–´ë–»ê²Œ ë˜ëŠ”ê±°ì§€? $$d_k$$ì°¨ì› ì§œë¦¬ë¡œ ë²¡í„°ê°€ ê·¸ ìì²´ë¡œ ë˜ K, V, Qê°€ ë˜ëŠ”ê±´ê°€?

â†’ FFN í›„ì˜ (seq_len, d_model)ì˜ MATRIXì„ ê° K, V, Që¡œ LINEAR PROJECTIONí•´ì„œ ì¬ì‚¬ìš©

ã„´. Encoderì—ì„œë„ stack layerê°€ N = 6 ì´ê³  decoder ì—ì„œë„ stack layerê°€ N = 6ì¸ë° ìš°ë¦¬ì˜ ì—­ì‚¬ì ì¸ í† ë¡  ì£¼ì œì¸ ì¸ì½”ë” ë””ì½”ë”ëŠ” ê° stackì—ì„œ íˆë“ ë²¡í„°ë¡œ ì—°ê²°ë˜ëŠ”ê°€ ì•„ë‹˜ ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ stackë§Œ ê°€ëŠ”ê°€ ê°€ ê¶ê¸ˆí•˜ë„¤ìš” wikidocsëŠ” í›„ìì²˜ëŸ¼ ê·¸ë ¤ì§€ë„¤ìš©

â†’ ë§¨ ìœ„ stackë§Œ ê°€ëŠ”ê²Œ ë§ë‹¤
ã„·.  3.2.3 ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€ë¡œ ë­˜ ì–´ë–»ê²Œ ë°”ê¿¨ë‹¤ëŠ”ê±´ì§€..? attention ê°’ì„ ë°”ê¾¼ê±´ê°€..? [MASK] ì´ëŸ° í† í°ìœ¼ë¡œ ë°”ê¾¸ë©´ ì™œ ì•ˆë ê¹Œ?

Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of **scaled dot-product attention by masking out (setting to âˆ’âˆ)** all values in the input of the softmax which correspond to illegal connections. See Figure 2.

â†’ ì •ë°©í–‰ë ¬ë¡œ ë‚˜ì˜¤ëŠ” attention valueë¥¼ ìœ„ ì§ê°ì‚¼ê°í˜•ì„ ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€ë¡œ ë°”ê¾¼ë“¯ ê·¸ë˜ì•¼ softmax ê°’ì´ 0ì´ ë¨

â†’[MASK] tokenìœ¼ë¡œ ë°”ê¾¸ë©´ ì–´ì°Œëë“  [MASK]ë€ í† í°ì´ ë“¤ì–´ê°„ ì±„ë¡œ í•™ìŠµì´ ë ê±°ì—¬ì„œ ì•„ì˜ˆ ì–´í…ì…˜ ë²¨ë¥˜ë¥¼ ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€ë¡œ í•´ì„œ softmaxë¥¼ 0ìœ¼ë¡œ ë°”ê¿”ì„œ í•™ìŠµì´ ì•ˆë˜ê²Œ í•˜ê¸°

ã„¹. 3.4. share embedding?

In our model, we share the **same weight matrix between the two embedding layers** and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by âˆšdmodel. ...?

ã…. label smoothing 
í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ì—ì„œ ì •ë‹µ ë ˆì´ë¸”ì„ 1, 0 ìœ¼ë¡œ ë‘ëŠ”ê²Œ ì•„ë‹ˆê³   1 - ì—¡ì‹¤ë¡ , 0 + ì—¡ì‹¤ë¡ ìœ¼ë¡œ ë‘ëŠ” ê²ƒ
1) ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•  ìˆ˜ ìˆìŒ
ëª¨ë¸ì´ íŠ¸ë ˆì´ë‹ ë°ì´í„°ì˜ GT ë ˆì´ë¸”ì— full probabilityë¥¼ í• ë‹¹í•˜ë©´, ì¼ë°˜í™”í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ
2) ê°€ì¥ í° logitê³¼ ì‘ì€ logitì˜ ì°¨ì´ë¥¼ í¬ê²Œ ë§Œë“¤ë©´, ëª¨ë¸ì´ adaptí•  ëŠ¥ë ¥ì„ ì¤„ì„

**3) ì¬ë°Œì—ˆë˜ ë¶€ë¶„**
. additive attentionë³´ë‹¤ dot-productê°€ í–‰ë ¬ì—°ì‚°ì´ê¸° ë•Œë¬¸ì— ë” íš¨ìœ¨ì ì´ë¼ëŠ” ì ....ìƒê°ì§€ë„ ëª»í•¨...
. self attention ì´ ì—¬ê¸°ì„œ ì²˜ìŒ ë‚˜ì˜¨ ê±´ ì•„ë‹ˆêµ¬ë‚˜ ì´ ë…¼ë¬¸ì´ ë‚˜ì˜¤ê¸° ì „ì— ë‚˜ì™”ë˜ ìœ ì‚¬í•œ ì‹œë„ë¥¼ í•œ ë¬´ìˆ˜íˆ ë§ì€ ë…¼ë¬¸ì´ ìˆêµ¬ë‚˜..
. stackì€ ë†’ì´ë¡œ ì˜¬ë¼ê°€ëŠ”ê±°ê³ , multi-headëŠ” ë‘ê»˜ë¼ê³  ìƒê°í•˜ë©´ ì´ ëª¨ë¸ì€ ì°¸ ë†’ì´ë„ ìŒ“ì•˜ê³  ë‘ê»ê²Œë„ ìŒ“ì•˜êµ¬ë‚˜ ì—¬ëŸ¬ ì°¨ì›ìœ¼ë¡œ ë§ì´ ìŒ“ì•˜ë„¤ ì–´ë–»ê²Œ ë³´ë©´ CNN ìŒ“ëŠ” ëŠë‚Œì´ë‘ë„ ë¹„ìŠ·í•˜ë‹¤
. self attentionì˜ ì¥ì ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì“´ ë¶€ë¶„

**4) ë…¼ë¬¸ êµ¬í˜„ ì‹œ ì£¼ì˜í•´ì•¼í•  ê²ƒ ê°™ì€ ë¶€ë¶„(ë…¼ë¬¸ ë³¸ë¬¸ ë³µë¶™)**

. scaled dot attention<br>
. self attention<br>
. stack self attention?<br>
. Residual block<br>
. multi-head self attention<br>
. masking<br>
. positional encoding<br>
. optimizer<br>
. warm-up step 

## ğŸ¤« ë…¼ë¬¸ê³¼ ë‹¤ë¥´ê²Œ êµ¬í˜„í•œ ë¶€ë¶„
- dataset : multi30k

## ğŸ¤­ ë…¼ë¬¸ êµ¬í˜„í•˜ë©´ì„œ ë°°ìš´ ì  / ëŠë‚€ ì 
- transformer êµ¬ì¡°ì— ëŒ€í•œ ì´í•´ : encoderì—ì„œì˜ queryê°€ decoderë¡œ ë„˜ì–´ê°€ëŠ” ë¶€ë¶„ì„ ì •í™•íˆ ì´í•´ ëª»í–ˆëŠ”ë° ë§ˆì§€ë§‰ layerì˜ attention valueë¥¼ ë„˜ê²¨ì„œ ì´ë¥¼ K, Vë¥¼ ê³±í•´ì„œ êµ¬í•˜ëŠ”ê±°êµ¬ë‚˜ ì´í•´ë¥¼ í•¨.
- `matmul`ê³¼ `bmm`ì˜ ì°¨ì´ : matmulì€ broadcasting ì²˜ë¦¬ ë¨
- stacked self-attentionì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ FCNì—ì„œ linearë¡œ í•œë²ˆ ì°¨ì›ì„ í‚¤ì› ë‹¤ê°€ ë‹¤ì‹œ ë‚®ì¶¤
- multi-head self-attentionì„ í•˜ê¸° ìœ„í•´ Q, K, Vë³„ë¡œ head_dimì„ ë§Œë“œëŠ”ê²Œ ì•„ë‹ˆë¼ hid_dimì„ ë§Œë“¤ê³  n_heads ë§Œí¼ ìë¥´ëŠ” ë°©ì‹
- label smoothing cross entropy íŠ¸ë¦­ê³¼ êµ¬í˜„
- `nll_loss`ì˜ `ignore_index`ì˜ ì˜ë¯¸
- `register_buffer`ì˜ ì‚¬ìš© ì´ìœ  : `model.parameters`ì— ì•ˆë‚˜ì˜¤ê²Œ í•˜ë ¤ê³ 
- `LambdaLR`ì˜ ì‚¬ìš© : schedulerëŠ” optimizerì˜ lrì— `*=` ì—°ì‚°ì´ ë˜ê¸° ë•Œë¬¸ì— optimizerì—ì„œ lrì€ 1ë¡œ í•´ì¤˜ì•¼í•¨ 
- broadcastingì„ ì‚¬ìš©í•œ src, trgì— ëŒ€í•œ mask êµ¬í˜„ ë°©ì‹
- `scheduler.step()`ì´ `optimizer.step()` ë’¤ë¡œ ì™€ì•¼ í•¨
